{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stages:\n",
    "# 1. Load a large set of geotiffs as xarray datasets\n",
    "# 2. Select a few 32x32 pixel chips from each xarray\n",
    "# 3. Generate embeddings for each chip\n",
    "# 4. Create two lists. One containing the geometries of the chips and the other containing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import get_context\n",
    "import numpy as np\n",
    "import random\n",
    "import rioxarray\n",
    "from shapely.geometry import box, Point\n",
    "from scipy import spatial\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import ipyleaflet as ipyl\n",
    "import ipywidgets as ipyw\n",
    "\n",
    "from gee.constants import S2_BANDS, REDUCERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xarray(file_name, chips_per_tile=2, patch_size=32):\n",
    "    band_names = [f\"{band}_{reducer}\" for band in S2_BANDS for reducer in REDUCERS]\n",
    "    data = rioxarray.open_rasterio(file_name)\n",
    "    data['band'] = band_names\n",
    "    data['time'] = datetime(2020, 1, 1)\n",
    "    data = data.transpose('y', 'x', 'band')\n",
    "    bboxes = []\n",
    "    mean_chips = []\n",
    "    std_chips = []\n",
    "    for i in range(chips_per_tile):\n",
    "        chip = extract_subset(data, patch_size)\n",
    "        # get the bounding box of the dataset\n",
    "        bbox = chip.rio.bounds()\n",
    "        mean_chip = get_reducer(chip, 'mean').to_numpy()\n",
    "        std_chip = get_reducer(chip, 'stdDev').to_numpy()\n",
    "        bboxes.append(bbox)\n",
    "        mean_chips.append(mean_chip)\n",
    "        std_chips.append(std_chip)\n",
    "    return mean_chips, std_chips, bbox\n",
    "\n",
    "def load_xarrays(path, max_files=-1, num_workers=12):\n",
    "    \"\"\"Load xarrays from a directory\"\"\"\n",
    "    \n",
    "    # get all the files in the directory\n",
    "    files = glob.glob(path + '*.tif')\n",
    "    # select a random subset of files\n",
    "    if max_files > 0:\n",
    "        files = random.sample(files, max_files)\n",
    "    # load the files into xarrays using multiprocessing\n",
    "    args = [(f, 1, 32) for f in files]\n",
    "    with get_context('fork').Pool(num_workers) as pool:\n",
    "        xarrays = pool.starmap(load_xarray, args)\n",
    "    return xarrays\n",
    "\n",
    "# take a random square subset of the data\n",
    "def extract_subset(xarr, size):\n",
    "    # extract a random patch of height and width size\n",
    "    x = random.randint(0, xarr.shape[1] - size)\n",
    "    y = random.randint(0, xarr.shape[0] - size)\n",
    "    return xarr[y:y+size, x:x+size, :]\n",
    "\n",
    "def get_rgb(xarr):\n",
    "    return xarr.sel(band=['B4_mean', 'B3_mean', 'B2_mean'])\n",
    "\n",
    "def get_reducer(xarr, reducer):\n",
    "    return xarr.sel(band=[f\"{band}_{reducer}\" for band in S2_BANDS])\n",
    "\n",
    "def unit_norm(chips):\n",
    "    # Means and standard deviations used for normalization\n",
    "    # The mean value bands and the standard deviation bands are normalized separately\n",
    "    # If this works, move these to a constants.py file\n",
    "    MEAN_MEAN = [336.802, 531.024, 468.612, 886.337, 1917.027, 2276.156, 2436.756, 2546.673, 1766.056, 983.634, 0.648, -0.614]\n",
    "    STD_MEAN = [131.763, 169.545, 219.578, 245.352, 317.504, 363.269, 421.290, 403.054, 484.412, 361.534, 0.122, 0.100]\n",
    "    MEAN_STD = [120.957, 141.433, 162.636, 176.621, 567.319, 708.051, 676.749, 709.860, 291.253, 244.241, 0.120, 0.076]\n",
    "    STD_STD = [51.520, 58.440, 85.941, 71.694, 177.075, 227.558, 238.055, 224.945, 126.288, 127.786, 0.047, 0.029]\n",
    "    # normalize the mean and stdDev chips\n",
    "    mean_chips = chips[:,:,:,:12]\n",
    "    std_chips = chips[:,:,:,12:]\n",
    "    mean_chips_norm = [np.clip((mean_chip - np.tile(MEAN_MEAN, (32, 32, 1))) / np.tile(STD_MEAN, (32, 32, 1)), -3, 3) for mean_chip in mean_chips]    \n",
    "    std_chips_norm = [np.clip((std_chip - np.tile(MEAN_STD, (32, 32, 1))) / np.tile(STD_STD, (32, 32, 1)), -3, 3) for std_chip in std_chips]\n",
    "    # return chips in array of shape (num_chips, 32, 32, 24)\n",
    "    return np.concatenate((mean_chips_norm, std_chips_norm), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '../../alabama/'\n",
    "files = glob.glob(data_directory + '*.tif')\n",
    "band_names = [f\"{band}_{reducer}\" for band in S2_BANDS for reducer in REDUCERS]\n",
    "chips_per_tile = 1\n",
    "patch_size = 32\n",
    "\n",
    "mean_chips = []\n",
    "std_chips = []\n",
    "bboxes = []\n",
    "\n",
    "num_files = 5000\n",
    "num_workers = 12\n",
    "input_data = load_xarrays(data_directory, num_files, num_workers)\n",
    "# concatenate mean and std chips together into shape 32x32x24\n",
    "chips = np.concatenate(([x[0][0] for x in input_data], [x[1][0] for x in input_data]), axis=-1)\n",
    "# normalize\n",
    "norm_chips = unit_norm(chips)\n",
    "\n",
    "# create a geodataframe of bounding boxes\n",
    "bboxes = [x[2] for x in input_data]\n",
    "geometries = gpd.GeoDataFrame(geometry=[box(*bbox) for bbox in bboxes])\n",
    "# convert from Alabama UTM to WGS84. \n",
    "# Not guaranteed to be accurate if embeddings are from multiple UTM zones\n",
    "geometries = geometries.set_crs(epsg=32616).to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = keras.backend.shape(z_mean)[0]\n",
    "        dim = keras.backend.shape(z_mean)[1]\n",
    "        epsilon = keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + keras.backend.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "model_path = '../models/vae_encoder_12channel_2022-11-08.h5'\n",
    "model = load_model(model_path, custom_objects={'Sampling': Sampling})\n",
    "decoder_path = '../models/vae_decoder_12channel_2022-11-08.h5'\n",
    "decoder = load_model(decoder_path, custom_objects={'Sampling': Sampling})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    #This is the model where we create the set of learned filters\n",
    "    def __init__(self, num_filters=64):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(24, num_filters, 3, stride=1, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters, 3, stride=1, padding=\"same\")\n",
    "        self.conv3 = nn.Conv2d(num_filters, num_filters, 3, stride=1, padding=\"same\")\n",
    "        self.conv4 = nn.Conv2d(num_filters, num_filters, 3, stride=1, padding=\"same\")\n",
    "        self.conv5 = nn.Conv2d(num_filters, num_filters, 3, stride=1, padding=\"same\")\n",
    "        self.conv6 = nn.Conv2d(num_filters, 24, 3, stride=1, padding=\"same\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x + F.leaky_relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(x + F.leaky_relu(self.conv3(x)), 2)\n",
    "        x = F.interpolate(x + F.leaky_relu(self.conv4(x)), x.shape[-1]*2)\n",
    "        x = F.interpolate(x + F.leaky_relu(self.conv5(x)), x.shape[-1]*2)\n",
    "        x = F.leaky_relu(self.conv6(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def encode(self, x, flatten=True):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x + F.leaky_relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(x + F.leaky_relu(self.conv3(x)), 2)\n",
    "        x = x + F.leaky_relu(self.conv4(x))\n",
    "        \n",
    "        if flatten:\n",
    "            return torch.flatten(x, start_dim = 1)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "device = \"cpu\" #change this to \"cuda\" for GPU or \"mps\" for Apple M1 chip\n",
    "\n",
    "autoencoder = torch.load(\"/Users/ckruse/Downloads/residual_autoencoder.pt\", map_location=device)\n",
    "\n",
    "X = torch.tensor(norm_chips, dtype=torch.float32, device=device) #convert a small batch to a torch tensor\n",
    "X = X.permute((0, 3, 1, 2)) #PyTorch is channels first, so we do a quick permute here\n",
    "\n",
    "preds = autoencoder.encode(X).detach().cpu().numpy() #get the encodings as a numpy array\n",
    "\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('/Users/ckruse/Downloads/test_general_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the outputs of the model after the convolutional layers\n",
    "conv_model = keras.Model(inputs=model.input, outputs=model.get_layer('flatten_2').output)\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = conv_model.predict(np.clip(chips[:,:,:,1:4] / 4000, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(preds.flatten(), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5),facecolor=(1,1,1))\n",
    "plt.hist(preds[0:100].flatten(), bins=100)\n",
    "plt.xlabel(\"Encoding value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(norm_chips[:,:,:,:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a leaflet map with all geometries\n",
    "# define map parameters\n",
    "map_kwargs = dict()\n",
    "\n",
    "map_kwargs['zoom'] = 4\n",
    "map_kwargs['zoom_control'] = False\n",
    "map_kwargs['attribution_control'] = True\n",
    "map_kwargs['scroll_wheel_zoom'] = True\n",
    "map_kwargs[\"no_wrap\"] = True\n",
    "map_kwargs[\"prefer_canvas\"] = True\n",
    "controls = ipyl.ScaleControl(options=[\"update_when_idle\"])\n",
    "\n",
    "# build mapbox basemap\n",
    "MAPBOX_TOKEN = 'pk.eyJ1IjoiZWFydGhyaXNlLWRldiIsImEiOiJjazFrMmRwM3Mwa2xkM2VxN3c5YnIxMXFiIn0.AzF5Q7NFuOAGZWkOr9n8CA'\n",
    "MAPBOX_URL = 'https://api.mapbox.com/styles/v1/mapbox/satellite-streets-v11/tiles/512/{z}/{x}/{y}@2x?access_token='\n",
    "mapbox_url = MAPBOX_URL + MAPBOX_TOKEN\n",
    "mapbox_layer = ipyl.TileLayer(url=mapbox_url, no_wrap=True, controls=controls, name=\"basemap\",\n",
    "                            attribution=\"© <a href='https://www.mapbox.com/about/maps/'>Mapbox</a> © <a href='http://www.openstreetmap.org/copyright'>OpenStreetMap</a> <strong><a>\")\n",
    "# build map\n",
    "m = ipyl.Map(basemap=mapbox_layer, **map_kwargs)\n",
    "\n",
    "# set layout height\n",
    "m.layout = ipyw.Layout(height='800px')\n",
    "# add geometries to map\n",
    "m.add_layer(ipyl.GeoJSON(data=geometries.__geo_interface__, style={'color': 'red'}))\n",
    "# set the map center to the center of the first geometry\n",
    "# get the centroid of the geodataframe bounding box\n",
    "centroid = geometries.geometry[0].centroid.coords[0]\n",
    "# get the lat and lon of the centroid\n",
    "lat = centroid[1]\n",
    "lon = centroid[0]\n",
    "m.center = (lat, lon)\n",
    "# set the appropriate zoom\n",
    "m.zoom = 14\n",
    "clicked_point = None\n",
    "# get the coordinates of a clicked point\n",
    "def get_coords(**kwargs):\n",
    "    # look up the geometry using a spatial index that contains contains the clicked point and return the index\n",
    "    if kwargs['type'] == 'click':\n",
    "        point = Point(\n",
    "            kwargs['coordinates'][1],\n",
    "            kwargs['coordinates'][0]\n",
    "        )\n",
    "        # get the index of the clicked point in the geodataframe\n",
    "        search_idx = geometries.sindex.intersection((point.bounds))\n",
    "        embedding = preds[search_idx[0]]\n",
    "        # compute the cosine similarity between the clicked point and all other points\n",
    "        cos_sim = [1 - spatial.distance.cosine(embedding, pred) for pred in preds]\n",
    "        # get the index of the point with the second highest cosine similarity\n",
    "        idx = np.argsort(cos_sim)[-2]\n",
    "        # create images for plotting\n",
    "        search_image = np.clip((norm_chips[search_idx[0],:,:,2::-1] + 2) / 5, 0,1)\n",
    "        #decoder_pred = decoder.predict(np.expand_dims(embedding, axis=0))[0]\n",
    "        #decoder_img = np.clip((decoder_pred[:,:,2::-1] + 2) / 5, 0,1)\n",
    "        match_img = np.clip((norm_chips[idx,:,:,2::-1] + 2) / 5, 0,1)\n",
    "        # clear any existing figure from notebook cell and then plot images\n",
    "        fig, ax = plt.subplots(1,3, figsize=(9,3), dpi=100)\n",
    "        ax[0].imshow(search_image)\n",
    "        ax[0].set_title('Search Image')\n",
    "        ax[0].axis('off')\n",
    "        ax[1].imshow(np.ones((1,1,3)))\n",
    "        #ax[1].set_title('Reconstruction')\n",
    "        ax[2].axis('off')\n",
    "        ax[1].imshow(match_img)\n",
    "        ax[1].set_title('Match Image')\n",
    "        ax[1].axis('off')\n",
    "        #plt.show()\n",
    "        # get the centroid of the geometry with the highest cosine similarity\n",
    "        centroid = geometries.geometry[idx].centroid.coords[0]\n",
    "        # get the lat and lon of the centroid\n",
    "        lat = centroid[1]\n",
    "        lon = centroid[0]\n",
    "        # set the map center to the centroid of the geometry with the highest cosine similarity\n",
    "        m.center = (lat, lon)\n",
    "# add a click event to the map\n",
    "m.on_interaction(get_coords)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('m1-plastics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d425afa4a959a86aa036beaa1a58ff3469f38e31f3ec97f5785c695b9108eced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
