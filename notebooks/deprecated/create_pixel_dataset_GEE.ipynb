{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pixel Dataset (Deprecated in favor of Descartes pipeline)\n",
    "This notebook is the source for downloading Sentinel data to produce inputs to the spectral classifier.\n",
    "\n",
    "## Inputs\n",
    "The notebook operates by loading a set of coordinates either from a geojson or csv. For each location in the list, it downloads a patch of width `RECT_WIDTH` across a specified period of time.\n",
    "\n",
    "Note: the sampling of TPA sites is different. It constructs a bounding box around the polygon geometry of a TPA site. The process is convoluted. Will need to do better sampling at some point.\n",
    "\n",
    "## Outputs\n",
    "### Raw Data (Patch Histories):\n",
    "This is a minimally-processed form of the data. It is a dictionary of arrays with a structure `[date][site_name][band][band_img]`. These dictionaries can then be processed into pixel vectors, or could also converted to 2D stacks of patches.\n",
    "\n",
    "### Pixel Vectors:\n",
    "The output is a list of vectors. `[num_vectors][bands]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import ee\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from scripts.get_s2_data_ee import get_history, get_history_polygon, get_pixel_vectors\n",
    "from scripts.viz_tools import visualize_history\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel 2 band descriptions\n",
    "band_descriptions = {\n",
    "    'B1': 'Aerosols, 442nm',\n",
    "    'B2': 'Blue, 492nm',\n",
    "    'B3': 'Green, 559nm',\n",
    "    'B4': 'Red, 665nm',\n",
    "    'B5': 'Red Edge 1, 704nm',\n",
    "    'B6': 'Red Edge 2, 739nm',\n",
    "    'B7': 'Red Edge 3, 779nm',\n",
    "    'B8': 'NIR, 833nm',\n",
    "    'B8A': 'Red Edge 4, 864nm',\n",
    "    'B9': 'Water Vapor, 943nm',\n",
    "    'B11': 'SWIR 1, 1610nm',\n",
    "    'B12': 'SWIR 2, 2186nm'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geojson(file_name):\n",
    "    \"\"\"Load points saved as a GeoJSON and return a dictionary\"\"\"\n",
    "    with open(os.path.join(DATA_DIR, file_name)) as f:\n",
    "        sites = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    sampling_df = pd.DataFrame({\n",
    "        'name': [file_name.split('_')[0] + '_' + str(index) for index in range(len(sites['features']))],\n",
    "        'lon': [site['geometry']['coordinates'][0] for site in sites['features']],\n",
    "        'lat': [site['geometry']['coordinates'][1] for site in sites['features']],\n",
    "        'coords': [site['geometry']['coordinates'][0:2] for site in sites['features']],\n",
    "    })\n",
    "    \n",
    "    return sampling_df\n",
    "\n",
    "def load_csv(file_name):\n",
    "    sampling_df = pd.read_csv(os.path.join(DATA_DIR, file_name), converters={'coords': eval})\n",
    "    \n",
    "    return sampling_df\n",
    "\n",
    "def sample_adjacent(tpa_sites, offset, direction='east'):\n",
    "    \"\"\"\n",
    "    Outputs a data frame of sampling locations based on a distance\n",
    "    and direction from each TPA site.\n",
    "    This can be used for adjacent site sampling, or to create \"random\" negative sites if the\n",
    "    offset distance is set further away from the TPA location.\n",
    "    Returns a data frame\n",
    "    \"\"\"\n",
    "    if  'east' in direction.lower():\n",
    "        adjacent_sites = pd.DataFrame({\n",
    "            'name': [f\"{name}_{direction.lower()}_{offset}\" for name in tpa_sites['name']],\n",
    "            'lon': [lon + offset for lon in tpa_sites['lon']],\n",
    "            'lat': [lat for lat in tpa_sites['lat']],\n",
    "            'coords': [[lon + offset, lat] for lon, lat in zip(tpa_sites['lon'], tpa_sites['lat'])]\n",
    "        })\n",
    "        \n",
    "    if  'west' in direction.lower():\n",
    "        adjacent_sites = pd.DataFrame({\n",
    "            'name': [f\"{name}_{direction.lower()}_{offset}\" for name in tpa_sites['name']],\n",
    "            'lon': [lon - offset for lon in tpa_sites['lon']],\n",
    "            'lat': [lat for lat in tpa_sites['lat']],\n",
    "            'coords': [[lon + offset, lat] for lon, lat in zip(tpa_sites['lon'], tpa_sites['lat'])]\n",
    "        })\n",
    "    \n",
    "    if  'north' in direction.lower():\n",
    "        adjacent_sites = pd.DataFrame({\n",
    "            'name': [f\"{name}_{direction.lower()}_{offset}\" for name in tpa_sites['name']],\n",
    "            'lon': [lon for lon in tpa_sites['lon']],\n",
    "            'lat': [lat + offset for lat in tpa_sites['lat']],\n",
    "            'coords': [[lon + offset, lat] for lon, lat in zip(tpa_sites['lon'], tpa_sites['lat'])]\n",
    "        })\n",
    "    \n",
    "    if  'south' in direction.lower():\n",
    "        adjacent_sites = pd.DataFrame({\n",
    "            'name': [f\"{name}_{direction.lower()}_{offset}\" for name in tpa_sites['name']],\n",
    "            'lon': [lon for lon in tpa_sites['lon']],\n",
    "            'lat': [lat - offset for lat in tpa_sites['lat']],\n",
    "            'coords': [[lon + offset, lat] for lon, lat in zip(tpa_sites['lon'], tpa_sites['lat'])]\n",
    "        })\n",
    "    \n",
    "    return adjacent_sites\n",
    "\n",
    "def save_patch_history(data, name, label_class):\n",
    "    first_date = list(patch_history.keys())[0]\n",
    "    first_site = list(patch_history[first_date].keys())[0]\n",
    "    num_pixels = np.shape(patch_history[first_date][first_site]['B2'])[0]\n",
    "    file_name = f\"{name}_raw_{num_months}_months_{start_date}\"\n",
    "    \n",
    "    visualize_history(data, file_path=os.path.join(OUTPUT_DIR, 'patch_histories', f\"{file_name}_{num_pixels}px_patch_history.png\"))\n",
    "    with open(os.path.join(OUTPUT_DIR, 'patch_histories', f\"{file_name}_{num_pixels}px_patch_history.pkl\"),\"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "    with open(os.path.join(OUTPUT_DIR, 'patch_histories', f\"{file_name}_{num_pixels}px_patch_history_labels.pkl\"),\"wb\") as f:\n",
    "        pickle.dump([label_class] * len(data), f)\n",
    "        \n",
    "        \n",
    "def create_pixel_vectors(patch_history, num_months, holdout=False):\n",
    "    # Decompose patch history into vectors\n",
    "    # Output is month, pixel, band_value\n",
    "    pixel_data = []\n",
    "    if not holdout:\n",
    "        for month in list(patch_history.keys())[:num_months]:\n",
    "            pixel_vectors, width, height = get_pixel_vectors(patch_history, month)\n",
    "            pixel_data.append(pixel_vectors)\n",
    "\n",
    "    else:\n",
    "        for month in list(patch_history.keys())[num_months:]:\n",
    "            pixel_vectors, width, height = get_pixel_vectors(patch_history, month)\n",
    "            pixel_data.append(pixel_vectors)\n",
    "    # flatten all pixel_vectors into a flat set of vectors\n",
    "    # num_vectors, num_bands\n",
    "    pixel_vectors = []\n",
    "    for month in pixel_data:\n",
    "        for pixel in month:\n",
    "            pixel_vectors.append(pixel)\n",
    "\n",
    "    print(np.shape(pixel_vectors)[0], \"pixel vectors\")\n",
    "    \n",
    "    return pixel_vectors\n",
    "\n",
    "def save_pixel_vectors(data, name, label_class):\n",
    "    file_name = f\"{name}_raw_{num_months}_months_{start_date}\"\n",
    "    with open(os.path.join(OUTPUT_DIR, 'pixel_vectors', f\"{file_name}_pixel_vectors.pkl\"),\"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "    with open(os.path.join(OUTPUT_DIR, 'pixel_vectors', f\"{file_name}_pixel_vector_labels.pkl\"),\"wb\") as f:\n",
    "        pickle.dump([label_class] * len(data), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sampling Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration:\n",
    "# Set directory where training site json files are located and files are saved\n",
    "# Set rect width for all patches that are not TPA sites\n",
    "DATA_DIR = '../data/sampling_locations'\n",
    "OUTPUT_DIR = '../data/training_data'\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TPA Polygon Sites from GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TPA dataset\n",
    "with open(os.path.join(DATA_DIR, 'tpa_points.geojson')) as f:\n",
    "    tpa_points = json.load(f)\n",
    "\n",
    "tpa_sites = pd.DataFrame({\n",
    "    'name': [site['properties']['Name'] for site in tpa_points['features']],\n",
    "    'lon': [site['geometry']['coordinates'][0] for site in tpa_points['features']],\n",
    "    'lat': [site['geometry']['coordinates'][1] for site in tpa_points['features']],\n",
    "    'area': [site['properties']['Surface_Ha'] for site in tpa_points['features']],\n",
    "    'daily_volume': [site['properties']['TOT_Kg/Day'] for site in tpa_points['features']],\n",
    "    'coords': [site['geometry']['coordinates'] for site in tpa_points['features']]\n",
    "})\n",
    "\n",
    "\n",
    "# Add earth engine TPA Polygons to TPA dataframe\n",
    "with open(os.path.join(DATA_DIR, 'tpa_polygons.geojson'), 'r') as f:\n",
    "    json_tpa = json.load(f)\n",
    "f.close()\n",
    "tpa_polygons = [ee.FeatureCollection([element]) for element in list(json_tpa['features'])]\n",
    "\n",
    "tpa_sites['polygons'] = tpa_polygons\n",
    "display(tpa_sites.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sampling Sites from GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_df = load_geojson('city_points_30.geojson')\n",
    "sampling_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Sites Adjacent to another List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacent_df = sample_adjacent(tpa_sites, 0.01, 'north')\n",
    "adjacent_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sampling Sites from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampling_df = load_csv('w_nusa_tenggara_v1.1_positives.csv')\n",
    "#coords = [[lon, lat] for lon, lat in zip(sampling_df['lon'], sampling_df['lat'])]\n",
    "#sampling_df['coords'] = coords\n",
    "sampling_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Sampling Sites to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_df.to_csv(os.path.join(DATA_DIR, 'w_nusa_tenggara_v1.1_positives.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECT_WIDTH = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Patch History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of patch histories\n",
    "# Each patch history is a dictionary with the format:\n",
    "# patch_history[date][site_name][band][band_img]\n",
    "# This function takes a while to run as it is extracting data from GEE\n",
    "\n",
    "site_list = sampling_df\n",
    "name = 'w_nusa_tenggara_v1.1_positives'\n",
    "num_months = 24\n",
    "start_date = '2019-01-01'\n",
    "\n",
    "patch_history = get_history(site_list['coords'], \n",
    "                            site_list['name'], \n",
    "                            RECT_WIDTH,\n",
    "                            num_months = num_months,\n",
    "                            start_date = start_date,\n",
    "                            cloud_mask = True)\n",
    "\n",
    "save_patch_history(patch_history, name, label_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TPA Polygon History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get patch histories for TPA sites\n",
    "num_months = 12\n",
    "start_date = '2020-01-01'\n",
    "tpa_patch_history = get_history_polygon(tpa_sites['coords'], \n",
    "                                        tpa_sites['name'], \n",
    "                                        tpa_sites['polygons'], \n",
    "                                        4 * RECT_WIDTH,\n",
    "                                        start_date = start_date,\n",
    "                                        num_months = num_months,\n",
    "                                       )\n",
    "save_patch_history(tpa_patch_history, 'tpa_sites', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pixel Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_vectors = create_pixel_vectors(patch_history, len(patch_history))\n",
    "save_pixel_vectors(pixel_vectors, name, label_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Pixel Vectors with a Holdout Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout_months refers to a strategy of holding out the last n months of data for validation\n",
    "# Set this value to the number of months you want to separate from the training data\n",
    "\n",
    "holdout_months = 3\n",
    "\n",
    "pixel_vectors = create_pixel_vectors(tpa_patch_history, len(tpa_patch_history) - holdout_months)\n",
    "save_pixel_vectors(pixel_vectors, 'tpa_train', 1)\n",
    "\n",
    "holdout_pixel_vectors = create_pixel_vectors(tpa_patch_history, -holdout_months, holdout=True)\n",
    "save_pixel_vectors(holdout_pixel_vectors, 'tpa_holdout', 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
