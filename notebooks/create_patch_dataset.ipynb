{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Patch Dataset\n",
    "This notebook is the source for downloading Sentinel data for the 2D patch classifier\n",
    "\n",
    "### Inputs\n",
    "The notebook operates by loading a set of coordinates either from a geojson or csv. For each location in the list, it downloads a patch of width `RECT_WIDTH` across a specified period of time.\n",
    "\n",
    "### Outputs:\n",
    "Multispectral patches with the structure `[num_patches, height, width, bands]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import ee\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from scripts.get_s2_data_ee import get_history, get_history_polygon, get_pixel_vectors\n",
    "from scripts.viz_tools import visualize_history, create_img_stack, band_descriptions\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points(file_name):\n",
    "    \"\"\"Load points saved as a GeoJSON and return a dictionary\"\"\"\n",
    "    with open(os.path.join(DATA_DIR, file_name)) as f:\n",
    "        sites = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    site_table = pd.DataFrame({\n",
    "        'name': [file_name.split('_')[0] + '_' + str(index) for index in range(len(sites['features']))],\n",
    "        'lon': [site['geometry']['coordinates'][0] for site in sites['features']],\n",
    "        'lat': [site['geometry']['coordinates'][1] for site in sites['features']],\n",
    "        'coords': [site['geometry']['coordinates'][0:2] for site in sites['features']],\n",
    "    })\n",
    "    \n",
    "    return site_table\n",
    "\n",
    "def visualize_patch_history(data, name):\n",
    "    first_date = list(patch_history.keys())[0]\n",
    "    first_site = list(patch_history[first_date].keys())[0]\n",
    "    num_pixels = np.shape(patch_history[first_date][first_site]['B2'])[0]\n",
    "    file_name = f\"{name}_patches_{num_months}_months_{start_date}\"\n",
    "    visualize_history(data, file_path=os.path.join(OUTPUT_DIR, 'patches', f\"{file_name}_{num_pixels}px_patches.png\"))\n",
    "        \n",
    "def save_patches(data, name, label_class):\n",
    "    num_pixels = np.shape(data)[1]\n",
    "    file_name = f\"{name}_patches_{num_months}_months_{start_date}\"\n",
    "    with open(os.path.join(OUTPUT_DIR, 'patches', f\"{file_name}_{num_pixels}px_patches.pkl\"),\"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "    with open(os.path.join(OUTPUT_DIR, 'patches', f\"{file_name}_{num_pixels}px_patch_labels.pkl\"),\"wb\") as f:\n",
    "        pickle.dump([label_class] * len(data), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sampling Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration:\n",
    "# Set directory where training site json files are located and files are saved\n",
    "# Set rect width for all patches that are not TPA sites\n",
    "DATA_DIR = '../data/sampling_locations'\n",
    "OUTPUT_DIR = '../data/training_data'\n",
    "    \n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site coordinates from candidate geojson\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'v12_java_bali_validated_positives.geojson'), 'r') as f:\n",
    "    positive_sites = json.load(f)['features']\n",
    "positive_coords = [site['geometry']['coordinates'] for site in positive_sites]\n",
    "positive_names = ['site_' + str(i) for i in range(len(positive_sites))]\n",
    "\n",
    "print(len(positive_coords), 'positive sites loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read site coordinates from candidate csv\n",
    "sampling_points = pd.read_csv(os.path.join(DATA_DIR, 'w_nusa_tenggara_v1.1_negatives.csv'), converters={'coords': eval})\n",
    "sampling_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sites from a sampling geojson\n",
    "sampling_points = load_points('city_points_30.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECT_WIDTH = 0.004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of patch histories\n",
    "# Each patch history is a dictionary with the format:\n",
    "# patch_history[date][site_name][band][band_img]\n",
    "# This function takes a while to run as it is extracting data from GEE\n",
    "\n",
    "filename = 'w_nusa_tenggara_v1.1_negatives'\n",
    "label_class = 0\n",
    "num_months = 12\n",
    "start_date = '2020-01-01'\n",
    "patch_history = get_history(sampling_points['coords'], \n",
    "                            sampling_points['name'], \n",
    "                            RECT_WIDTH,\n",
    "                            num_months=num_months,\n",
    "                            start_date=start_date,\n",
    "                            cloud_mask=True)\n",
    "\n",
    "visualize_patch_history(patch_history, filename)\n",
    "patches = create_img_stack(patch_history)\n",
    "save_patches(patches, filename, label_class)\n",
    "print(len(patches), 'images extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example from Bali/Java Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "java_bootstrap = pd.read_csv('../data/model_outputs/candidate_sites/v12_java_2D_candidates_0.4_threshold.csv')\n",
    "del java_bootstrap['Unnamed: 0']\n",
    "java_bootstrap_coords = [[lon, lat] for lon, lat in zip(java_bootstrap['lon'], java_bootstrap['lat'])]\n",
    "java_bootstrap_names = ['java_' + str(index) for index in range(len(java_bootstrap))]\n",
    "\n",
    "java_curated = pd.read_csv('../data/model_outputs/candidate_sites/v12_java_2D_candidates_0.4_threshold_validated.csv')\n",
    "java_positive_index = java_curated['label'] == 1\n",
    "java_positive_coords = np.array(java_bootstrap_coords)[java_positive_index]\n",
    "java_positive_names = np.array(java_bootstrap_names)[java_positive_index]\n",
    "\n",
    "java_negative_index = java_curated['label'] == 0\n",
    "java_negative_coords = np.array(java_bootstrap_coords)[java_negative_index]\n",
    "java_negative_names = np.array(java_bootstrap_names)[java_negative_index]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'name': java_positive_names,\n",
    "    'lon': java_positive_coords[:,0],\n",
    "    'lat': java_positive_coords[:,1],\n",
    "    'coords': [[coord[0], coord[1]] for coord in java_positive_coords]\n",
    "}).to_csv('../data/sampling_locations/v12_java_validated_positives.csv', index=False)\n",
    "\n",
    "\n",
    "pd.DataFrame({\n",
    "    'name': java_negative_names,\n",
    "    'lon': java_negative_coords[:,0],\n",
    "    'lat': java_negative_coords[:,1],\n",
    "    'coords': [[coord[0], coord[1]] for coord in java_negative_coords]\n",
    "}).to_csv('../data/sampling_locations/v12_java_validated_negatives.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
