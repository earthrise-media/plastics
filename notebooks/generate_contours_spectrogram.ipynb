{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Site Monitoring and Contour Generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import rasterio as rs\n",
    "import shapely\n",
    "from tensorflow import keras\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scripts import dl_utils\n",
    "from scripts.dl_utils import predict_spectrogram, rect_from_point"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download Single Site"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Image upsampling factor. Makes for smoother contours\n",
    "SCALE = 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_ensemble(folder_path):\n",
    "    model_files = [file for file in os.listdir(folder_path) if '.h5' in file]\n",
    "    model_list = []\n",
    "    for file in model_files:\n",
    "        model_list.append(keras.models.load_model(os.path.join(folder_path,file)))\n",
    "    return model_list\n",
    "\n",
    "def predict_ensemble(pairs, model_list):\n",
    "    ensemble_preds = []\n",
    "    for pair in pairs:\n",
    "        pred_stack = []\n",
    "        for ensemble_model in model_list:\n",
    "            pred_stack.append(predict_spectrogram(pair, ensemble_model, unit_norm=True))\n",
    "        ensemble_preds.append(np.median(pred_stack, axis=0))\n",
    "    return ensemble_preds\n",
    "\n",
    "def generate_contours(preds, dates, threshold=0.5, plot=False):\n",
    "    \"\"\"\n",
    "    Generate a list of contours for a set of predictions\n",
    "    Inputs\n",
    "        - preds: A list of numpy prediction arrays\n",
    "        - dates: A list of dates for each scene in prediction list\n",
    "        - threshold: Value under which pixels are masked. Given that the heatmaps are first\n",
    "                     blurred, it is recommended to set this value lower than in blob detection\n",
    "        - plot: Visualize outputs if True\n",
    "    Outputs\n",
    "        - A list of contours. Each prediction frame has a separate list of contours. \n",
    "          Contours are defined as (x,y) pairs\n",
    "        - A list of dates for instance when contours were generated\n",
    "    \"\"\"\n",
    "    \n",
    "    img_size = preds[0].shape\n",
    "    \n",
    "    # Set a prediction threshold. Given that the heatmaps are blurred, it is recommended\n",
    "    # to set this value lower than you would in blob detection\n",
    "    contour_list = []\n",
    "    date_list = []\n",
    "    for pred, date in zip(preds, dates):\n",
    "        # If a scene is masked beyond a threshold, don't generate contours\n",
    "        masked_percentage = np.sum(pred.mask / np.size(pred.mask))\n",
    "        if masked_percentage < 0.1:\n",
    "            pred = np.array(Image.fromarray(pred).resize((img_size[0] * SCALE, img_size[1] * SCALE), Image.BICUBIC))\n",
    "            # OpenCV works best with ints in range (0,255)\n",
    "            input_img = (pred*255).astype(np.uint8)\n",
    "            # Blur the image to minimize the influence of single-pixel or mid-value model outputs\n",
    "            blurred = cv2.GaussianBlur(input_img, (8 * SCALE + 1,8 * SCALE + 1), cv2.BORDER_DEFAULT)\n",
    "            # Set all values below a threshold to 0\n",
    "            _, thresh = cv2.threshold(blurred, int(threshold * 255), 255, cv2.THRESH_TOZERO)\n",
    "            # Note that cv2.RETR_CCOMP returns a hierarchy of parent and child contours\n",
    "            # Needed for fixing the case with polygon holes \n",
    "            # https://docs.opencv.org/master/d9/d8b/tutorial_py_contours_hierarchy.html\n",
    "            contours, hierarchy = cv2.findContours(thresh, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            contours = [contour for contour in contours if cv2.contourArea(contour) > 40 * SCALE]\n",
    "            contour_list.append(contours)\n",
    "            date_list.append(date)\n",
    "            \n",
    "            if plot:\n",
    "                plt.figure(figsize=(16,4), dpi=150)\n",
    "                plt.subplot(1,4,1)\n",
    "                plt.imshow(input_img, vmin=0, vmax=255)\n",
    "                plt.title('Pred')\n",
    "                plt.axis('off')\n",
    "                plt.subplot(1,4,3)\n",
    "                plt.imshow(thresh, vmin=0, vmax=255)\n",
    "                plt.title('Thresholded Blur')\n",
    "                plt.axis('off')\n",
    "                plt.subplot(1,4,2)\n",
    "                plt.imshow(blurred, vmin=0, vmax=255)\n",
    "                plt.title('Blurred')\n",
    "                plt.axis('off')\n",
    "                plt.subplot(1,4,4)\n",
    "                three_channel_preds = np.stack((blurred, blurred, blurred), axis=-1)\n",
    "                preds_contour_img = cv2.drawContours(three_channel_preds, contours, -1, (255, 0, 0), SCALE)\n",
    "                plt.imshow(preds_contour_img /255, vmin=0, vmax=255)\n",
    "                plt.title(f\"{len(contours)} separate contours\")\n",
    "                plt.axis('off')\n",
    "                plt.suptitle(date)\n",
    "                plt.show()\n",
    "                \n",
    "    return contour_list, date_list\n",
    "\n",
    "def generate_polygons(contour_list, bounds_list, preds, plot=False):\n",
    "    \"\"\"\n",
    "    Convert a list of coordinates into georeferenced shapely polygons\n",
    "    Inputs\n",
    "        - List of contours\n",
    "        - List of patch coordinate boundaries\n",
    "    Returns\n",
    "        - A list of shapely MultiPolygons. One for each coordinate in the input list\n",
    "    \"\"\"\n",
    "    contour_multipolygons = []\n",
    "    for contours, bounds in zip(contour_list, bounds_list):\n",
    "        # Define patch coordinate bounds to set pixel scale\n",
    "        bounds = shapely.geometry.Polygon(bounds).bounds\n",
    "        transform = rs.transform.from_bounds(*bounds, preds[0].shape[0] * SCALE, preds[0].shape[1] * SCALE)\n",
    "        polygon_coords = []\n",
    "        for contour in contours:\n",
    "            # Convert from pixels to coords\n",
    "            contour_coords = []\n",
    "            for point in contour[:,0]:\n",
    "                lon, lat = rs.transform.xy(transform, point[1], point[0])\n",
    "                contour_coords.append([lon, lat])\n",
    "            if len(contour_coords) > 1:\n",
    "                # Close the loop\n",
    "                contour_coords.append(contour_coords[0])\n",
    "                # Add individual contour to list of contours for the month\n",
    "                polygon_coords.append(contour_coords)\n",
    "\n",
    "        contour_polygons = []\n",
    "        for coord in polygon_coords:\n",
    "            poly = shapely.geometry.Polygon(coord)\n",
    "            # A single line of pixels will be recognized as a line rather than a polygon\n",
    "            # Inflate the area by a small amount to create a polygon\n",
    "            if poly.area == 0:\n",
    "                poly = poly.buffer(0.00002)\n",
    "            contour_polygons.append(poly)\n",
    "        multipolygon = shapely.geometry.MultiPolygon(contour_polygons)\n",
    "        # Currently, \"holes\" in a polygon are seen as separate contours.\n",
    "        # This means that there will be overlapping polygons. Shapely can \n",
    "        # detect this case, but can't fix it automatically. To rectify, the\n",
    "        # unary_union operator and .buffer(0) hack removes interior polygons.\n",
    "        if not multipolygon.is_valid:\n",
    "            multipolygon = multipolygon.buffer(0)\n",
    "        if plot:\n",
    "            display(multipolygon)\n",
    "        contour_multipolygons.append(multipolygon)\n",
    "    return contour_multipolygons\n",
    "\n",
    "def mask_predictions(preds, threshold=0.1):\n",
    "    # Create a median prediction mask\n",
    "    mask = np.median(preds, axis=0)\n",
    "    # Set a threshold for masked pixels. Value should be set quite low\n",
    "    mask_thresh = threshold\n",
    "    mask[mask < mask_thresh] = 0\n",
    "    mask[mask > mask_thresh] = 1\n",
    "    # mask predictions\n",
    "    masked_preds = [np.ma.multiply(pred, mask) for pred in preds]\n",
    "    return masked_preds\n",
    "\n",
    "def resolve_overlapping_contours(confirmed_sites, contour_df):\n",
    "    \"\"\"\n",
    "    This is not a function I am proud of writing. The intention is to \n",
    "    look at a dataframe of contours, find sets of overlapping contours,\n",
    "    and then assign a contour to the site which is nearest. It seems that\n",
    "    this could be done with less code and fewer loops.\n",
    "    Inputs:\n",
    "      - confirmed_sites: the original dataframe of site names and coordinates\n",
    "      - contour_gdf: a dataframe of contours for each site through time\n",
    "    Returns:\n",
    "      - a contour dataframe with no overlapping contours.\n",
    "    \"\"\"\n",
    "    rect_height = 0.02\n",
    "    rects = []\n",
    "    coords = [[site.x, site.y] for site in confirmed_sites['geometry']]\n",
    "    for candidate in coords:\n",
    "        rect = dl_utils.rect_from_point(candidate, rect_height)\n",
    "        rects.append(shapely.geometry.Polygon(rect['coordinates'][0]))\n",
    "\n",
    "    confirmed_sites['rect'] = rects\n",
    "    names = confirmed_sites['name']\n",
    "    overlapping = []\n",
    "    for name, rect in zip(names, rects):\n",
    "        overlapping.append([rect.overlaps(other_rects) for other_rects in rects])\n",
    "    confirmed_sites['overlap'] = [list(confirmed_sites['name'][overlap]) for overlap in overlapping]\n",
    "\n",
    "    contour_df['updated_geometry'] = [None for _ in range(len(contour_df))]\n",
    "\n",
    "    all_contours = []\n",
    "    contour_indices = []\n",
    "    for site in tqdm(names):\n",
    "        site_contours = list(contour_df[contour_df['name'] == site]['geometry'])\n",
    "        site_indices = list(contour_df[contour_df['name'] == site]['geometry'].index)\n",
    "        site_dates = list(contour_df[contour_df['name'] == site]['date'])\n",
    "        site_center = confirmed_sites[confirmed_sites['name'] == site]['geometry'].item()\n",
    "        overlapping_sites = list(confirmed_sites[confirmed_sites['name'] == site]['overlap'])[0]\n",
    "        \n",
    "        # iterate through each site with a rect that overlaps the site of interest rect\n",
    "        for external_site in overlapping_sites:\n",
    "            external_contours = list(contour_df[contour_df['name'] == external_site]['geometry'])\n",
    "            external_dates = list(contour_df[contour_df['name'] == external_site]['date'])\n",
    "            external_center = confirmed_sites[confirmed_sites['name'] == external_site]['geometry'].item()\n",
    "            \n",
    "            # iterate through each date where the site has a contour\n",
    "            for index, date in enumerate(site_dates):\n",
    "                \n",
    "                # check if overlapping site has a contour at that date\n",
    "                if date in external_dates:\n",
    "                    site_index = external_dates.index(date)\n",
    "                    \n",
    "                    # sometimes contours can be none. Check if contours exist for both\n",
    "                    if site_contours[index] and external_contours[site_index]:\n",
    "                        \n",
    "                        # check if the multipolygon contours overlap\n",
    "                        try:\n",
    "                            contours_overlap = site_contours[index].overlaps(external_contours[site_index])\n",
    "                            if contours_overlap == True:\n",
    "                                # if the contours overlap, create a new list of polygons\n",
    "                                site_polygon_list = []\n",
    "\n",
    "                                # make sure sites are multipolygons rather than polygons\n",
    "                                if type(external_contours[site_index]) != shapely.geometry.multipolygon.MultiPolygon:\n",
    "                                    external_contours[site_index] = shapely.geometry.MultiPolygon([external_contours[site_index]])\n",
    "\n",
    "                                # for each polygon in the external site multipolygon\n",
    "                                for external_polygon in external_contours[site_index].geoms:\n",
    "                                    # check each polygon in the site multipolygon to see if they overlap\n",
    "                                    if type(site_contours[index]) != shapely.geometry.multipolygon.MultiPolygon:\n",
    "                                        site_contours[index] = shapely.geometry.MultiPolygon([site_contours[index]])\n",
    "                                    for site_polygon in site_contours[index].geoms:\n",
    "\n",
    "                                        # if the site polygon overlaps, check which rect center is nearest\n",
    "                                        if site_polygon.overlaps(external_polygon):\n",
    "                                            site_centroid = site_polygon.centroid\n",
    "                                            site_distance = site_center.distance(site_centroid)\n",
    "                                            external_centroid = external_polygon.centroid\n",
    "                                            external_distance = external_center.distance(external_centroid)\n",
    "                                            if site_distance > external_distance:\n",
    "                                                #print(f\"Site {site} overlaps {external_site} on {date}, {site}'s polygon is closer\")\n",
    "                                                try:\n",
    "                                                    site_contours[index] -= site_polygon\n",
    "                                                except Exception as e:\n",
    "                                                    print(e)\n",
    "                                            # else:\n",
    "                                                #print(f\"Site {site} overlaps {external_site} on {date}, {external_site}'s polygon is closer\")\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "\n",
    "        all_contours += site_contours\n",
    "        contour_indices += site_indices\n",
    "\n",
    "    resolved_df = gpd.GeoDataFrame({\n",
    "            'name': contour_df['name'][contour_indices], \n",
    "            'date': contour_df['date'][contour_indices]}, \n",
    "            geometry=all_contours).set_crs('EPSG:4326')\n",
    "\n",
    "    resolved_df['area (km^2)'] = resolved_df['geometry'].to_crs('epsg:3395').map(lambda p: p.area / 10**6 if p != None else None)\n",
    "    \n",
    "    return resolved_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Rect needs to be large enough to cover big sites. \n",
    "# But large rects take longer to process and increase false positive likelihood\n",
    "RECT_WIDTH = 0.008\n",
    "START_DATE = '2016-06-01'\n",
    "END_DATE = '2021-06-01'\n",
    "MOSAIC_PERIOD = 3\n",
    "SPECTROGRAM_INTERVAL = 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#model_name = 'spectrogram_v0.0.11_2021-07-13'\n",
    "#model = keras.models.load_model(f'../models/{model_name}.h5')\n",
    "\n",
    "ensemble_name = 'v0.0.11_ensemble-8-25-21'\n",
    "model_list = load_ensemble(f'../models/{ensemble_name}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "confirmed_sites_file = 'v12_bali_validated_positives'\n",
    "confirmed_sites = gpd.read_file(f\"../data/sampling_locations/{confirmed_sites_file}.geojson\")\n",
    "coords = [[site.x, site.y] for site in confirmed_sites['geometry']]\n",
    "#names = [f'java_{index}' for index, site in enumerate(confirmed_sites['geometry'])]\n",
    "#confirmed_sites['id'] = names\n",
    "names = confirmed_sites['name']\n",
    "display(confirmed_sites.head())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize a contour GeoDataFrame. I'm not certain that EPSG:4326 is the correct coordinate system.\n",
    "# Coordinate system must be set in order to compute polygon areas\n",
    "contour_gdf = gpd.GeoDataFrame(columns=['geometry', 'area (km^2)', 'date', 'name']).set_crs('EPSG:4326')\n",
    "for coord, name in zip(tqdm(coords[:2]), names[:2]):\n",
    "    try:\n",
    "        # Download data\n",
    "        mosaics, metadata = dl_utils.download_mosaics(\n",
    "            rect_from_point(coord, RECT_WIDTH), START_DATE, END_DATE, MOSAIC_PERIOD, method='min')\n",
    "        pairs = dl_utils.pair(mosaics, SPECTROGRAM_INTERVAL)\n",
    "\n",
    "        # Generate predictions\n",
    "        #preds = [predict_spectrogram(pair, model) for pair in pairs] # single model\n",
    "        preds = predict_ensemble(pairs, model_list) # ensemble model\n",
    "        preds = mask_predictions(preds, threshold=0.1)\n",
    "\n",
    "        # Generate contours and polygons\n",
    "        dates = dl_utils.get_starts(START_DATE, END_DATE, 3, 2)[SPECTROGRAM_INTERVAL:]\n",
    "        bounds = [sample['wgs84Extent']['coordinates'][0][:-1] for sample in metadata[SPECTROGRAM_INTERVAL:]]\n",
    "        contours, contour_dates = generate_contours(preds, dates, threshold=0.2, plot=False)\n",
    "        polygons = generate_polygons(contours, bounds, preds, plot=False)\n",
    "\n",
    "        # Write contours to a GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame(geometry=polygons).set_crs('EPSG:4326')\n",
    "        gdf['date'] = [datetime.datetime.fromisoformat(date) for date in contour_dates]\n",
    "        \n",
    "        # Calculate contour area. I'm not certain this is a valid technique for calculating area\n",
    "        gdf['area (km^2)'] = gdf['geometry'].to_crs('epsg:3395').map(lambda p: p.area / 10**6)\n",
    "        gdf['name'] = [name for _ in range(len(contour_dates))]\n",
    "        contour_gdf = contour_gdf.append(gdf, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(name, 'failed')\n",
    "        print(e)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "resolved_df = resolve_overlapping_contours(confirmed_sites, contour_gdf)\n",
    "resolved_df.to_file(f'../data/model_outputs/site_contours/{confirmed_sites_file}_masked_upsampled_{SCALE}_contours_model_{ensemble_name}.geojson', driver='GeoJSON')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('venv-plastics': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "interpreter": {
   "hash": "4a01d8cce8e3093998fa99b03bade8c0e7fde9c8096298c7b656f9b435102d6b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}