{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(positive=True):\n",
    "    data_path = '../../data/training_data/segmentation_masks/data'\n",
    "    label_path = '../../data/training_data/segmentation_masks/masks'\n",
    "    if positive:\n",
    "        data_files = [f for f in sorted(os.listdir(data_path)) if '.pkl' in f and 'neg' not in f]\n",
    "        label_files = [f for f in sorted(os.listdir(label_path)) if '.png' in f and 'neg' not in f]\n",
    "    else:\n",
    "        data_files = [f for f in sorted(os.listdir(data_path)) if '.pkl' in f and 'neg' in f]\n",
    "        label_files = [f for f in sorted(os.listdir(label_path)) if '.png' in f and 'neg' in f]\n",
    "\n",
    "    # Load the data\n",
    "    data = []\n",
    "    for f in data_files:\n",
    "        with open(os.path.join(data_path, f), 'rb') as frame:\n",
    "            data.append(pickle.load(frame))\n",
    "    data = np.array(data)\n",
    "    img_size = data[0].shape\n",
    "\n",
    "    # load the labels\n",
    "    labels = []\n",
    "    for f in label_files:\n",
    "        labels.append(plt.imread(os.path.join(label_path, f))[:,:,0])\n",
    "    labels = np.array(labels)\n",
    "    return data, labels\n",
    "\n",
    "def augment_data(x, y, iterations=1000):\n",
    "    # create albumentations augmentation set\n",
    "    aug = A.Compose([\n",
    "        A.RandomSizedCrop(min_max_height=(24, 38), height=48, width=48, p=1),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.25, scale_limit=0.5, rotate_limit=180, interpolation=1, border_mode=4, p=1),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.OneOf(\n",
    "                [A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03), \n",
    "                A.GridDistortion(p=0.5),A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=.1)], \n",
    "            p=0.8)\n",
    "    ])\n",
    "\n",
    "    # It would be nice to have this as a generator passed to the train function\n",
    "    # This was challenging though. Instead, I'm pregenerating a large dataset\n",
    "    # and then training on it\n",
    "    aug_x = []\n",
    "    aug_y = []\n",
    "    for _ in range(iterations):\n",
    "        for image, mask in zip(x, y):\n",
    "            augmented = aug(image=image, mask=mask)\n",
    "            aug_x.append(augmented['image'])\n",
    "            aug_y.append(augmented['mask'])\n",
    "    aug_x = np.array(aug_x)\n",
    "    aug_y = np.array(aug_y)\n",
    "    \n",
    "    return aug_x, aug_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data, positive_labels = load_data(positive=True)\n",
    "negative_data, negative_labels = load_data(positive=False)\n",
    "aug_pos_data, aug_pos_labels = augment_data(positive_data, positive_labels, iterations=100)\n",
    "aug_neg_data, aug_neg_labels = augment_data(negative_data, negative_labels, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for training\n",
    "x = np.concatenate((aug_pos_data, aug_neg_data))\n",
    "x = np.clip(x / 3000, 0, 1)\n",
    "y = np.expand_dims(np.copy(np.concatenate((aug_pos_labels, aug_neg_labels))).astype(int), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.choice(range(len(x)), 5, replace=False):\n",
    "    plt.figure(figsize=(10,5), dpi=100)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(x[i,:,:,3:0:-1])\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(y[i,:,:])\n",
    "    plt.axis('off')\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model. Code from keras example\n",
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (12,))\n",
    "    # max_conv is a variable that sets the max number of filters in a layer\n",
    "    max_conv = 256\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 12, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [max_conv // 4, max_conv // 2, max_conv]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [max_conv, max_conv // 2, max_conv // 4, max_conv // 8]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Build model\n",
    "resolution = 48\n",
    "model = get_model((resolution,resolution), 2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, smooth=1e-6, gamma=2):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.name = 'NDL'\n",
    "        self.smooth = smooth\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true, y_pred = tf.cast(\n",
    "            y_true, dtype=tf.float32), tf.cast(y_pred, tf.float32)\n",
    "        nominator = 2 * \\\n",
    "            tf.reduce_sum(tf.multiply(y_pred, y_true)) + self.smooth\n",
    "        denominator = tf.reduce_sum(\n",
    "            y_pred ** self.gamma) + tf.reduce_sum(y_true ** self.gamma) + self.smooth\n",
    "        result = 1 - tf.divide(nominator, denominator)\n",
    "        return result\n",
    "\n",
    "#loss = DiceLoss()\n",
    "loss = \"sparse_categorical_crossentropy\"\n",
    "model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam())\n",
    "train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "model.fit(x, y, batch_size=batch_size, epochs=epochs, initial_epoch=len(train_loss))\n",
    "train_loss += model.history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5), dpi=100, facecolor=(1,1,1))\n",
    "plt.plot(train_loss, label='Train Acc')\n",
    "#plt.plot(test_accuracy, c='r', label='Val Acc')\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Network Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "version_number = '0.1'\n",
    "current_date = date.today()\n",
    "model_name = f\"unet_{model.input_shape[1]}px_v{version_number}_{current_date.isoformat()}\"\n",
    "\n",
    "#assert not os.path.exists('../../models/' + model_name + '.h5'), f\"Model of name {model_name} already exists\"\n",
    "data_path = '../../data/training_data/segmentation_masks/data'\n",
    "data_files = [f for f in sorted(os.listdir(data_path)) if '.pkl' in f and 'neg' not in f]\n",
    "with open('../../models/' + model_name + '_config.txt', 'w') as f:\n",
    "    f.write('Input Data:\\n')\n",
    "    [f.write('\\t' + file + '\\n') for file in data_files]\n",
    "    f.write('\\n\\nAugmentation Parameters:\\n')\n",
    "    # TODO: aug is only defined within the augmentation function\n",
    "    for elem in aug._to_dict()['transforms']:\n",
    "        f.write(f\"{json.dumps(elem, indent=True)}\")\n",
    "    #for k, v in zip(aug._to_dict().keys(), aug._to_dict().values()):\n",
    "    #    f.write(f\"\\t{k}: {v}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nBatch Size: {batch_size}\")\n",
    "    f.write(f\"\\nLoss Function: {loss}\")\n",
    "    f.write(f\"\\nTraining Epochs: {len(train_loss)}\")\n",
    "    f.write(f\"\\nFinal Loss: {train_loss[-1]:.3f}\")\n",
    "        \n",
    "\n",
    "model.save(f'../../models/{model_name}.h5')\n",
    "print(f\"Saved to ../../models/{model_name}.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on test data\n",
    "samples = np.random.randint(0, len(x), size=10)\n",
    "for sample_num in samples:\n",
    "    num_plots = 4\n",
    "    pred =  model.predict(x[sample_num:sample_num+1])[0,:,:,1]\n",
    "    plt.figure(figsize=(9,3), dpi=75)\n",
    "    plt.subplot(1,4,1)\n",
    "    rgb = x[sample_num, :, :, 3:0:-1]\n",
    "    plt.axis('off')\n",
    "    plt.imshow(rgb)\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(y[sample_num], cmap='RdBu_r')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,4,2)\n",
    "    combo = np.copy(rgb)\n",
    "    combo[:,:,0] = combo[:,:,0] + pred\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.clip(combo, 0, 1))\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(pred, cmap='RdBu_r', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d425afa4a959a86aa036beaa1a58ff3469f38e31f3ec97f5785c695b9108eced"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('m1-plastics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
