{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize as sklearn_normalize\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (np.array(x)) / (3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '../data/training_sites'\n",
    "\n",
    "with open(os.path.join(train_data_dir, \"negative_data_toa.pkl\"), 'rb') as file:\n",
    "    x_negative = np.array(pickle.load(file))\n",
    "file.close()\n",
    "y_negative = np.zeros(len(x_negative))\n",
    "\n",
    "with open(os.path.join(train_data_dir, \"positive_data_toa.pkl\"), 'rb') as file:\n",
    "    x_positive = np.array(pickle.load(file))\n",
    "file.close()\n",
    "\n",
    "with open(os.path.join(train_data_dir, \"positive_data_test_toa.pkl\"), 'rb') as file:\n",
    "    x_positive_test = np.array(pickle.load(file))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_train = (x_positive[:,7] - x_positive[:,3]) / (x_positive[:,7] + x_positive[:,3])\n",
    "ndvi_test = (x_positive_test[:,7] - x_positive_test[:,3]) / (x_positive_test[:,7] + x_positive_test[:,3])\n",
    "\n",
    "lower_bound = -0.1\n",
    "upper_bound = 0.5\n",
    "index_train = np.logical_and(ndvi_train > lower_bound, ndvi_train < upper_bound)\n",
    "index_test = np.logical_and(ndvi_test > lower_bound, ndvi_test < upper_bound)\n",
    "\n",
    "x_positive = x_positive[index_train]\n",
    "y_positive = np.ones(len(x_positive))\n",
    "x_positive_test = x_positive_test[index_test]\n",
    "y_positive_test = np.ones(len(x_positive_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((x_positive, x_negative))\n",
    "y = np.concatenate((y_positive, y_negative))\n",
    "\n",
    "x, y = shuffle(x, y, random_state=42)\n",
    "x = normalize(x)\n",
    "x_positive_test = normalize(x_positive_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "print(\"Num Train:\\t\\t\", len(x_train))\n",
    "print(\"Num Test:\\t\\t\", len(x_test))\n",
    "print(f\"Percent Negative Train:\\t {100 * sum(y_train == 0.0) / len(y_train):.1f}\")\n",
    "print(f\"Percent Negative Test:\\t {100 * sum(y_test == 0.0) / len(y_test):.1f}\")\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "x_positive_test = np.expand_dims(x_positive_test, -1)\n",
    "\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_positive_test = keras.utils.to_categorical(y_positive_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../models/model_65_month_filtered_toa-12-09-2020.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_set_samples = 5000\n",
    "random_test_indices = np.random.choice(len(x_test), num_test_set_samples)\n",
    "\n",
    "inputs = np.concatenate((x_positive_test, x_test[random_test_indices]))\n",
    "preds = model.predict(inputs)\n",
    "labels = np.concatenate((y_positive_test, y_test[random_test_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = []\n",
    "for sample in inputs:\n",
    "    sample_rgb = np.zeros((1,1,3))\n",
    "    sample_rgb[0,0,:] = [sample[3], sample[2], sample[1]]\n",
    "    rgb.append(sample_rgb)\n",
    "rgb_min = np.array(rgb).min()\n",
    "rgb_max = np.array(rgb).max()\n",
    "rgb = np.array([(np.array(element) - rgb_min) / (rgb_max - rgb_min) for element in rgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile TP, FP, TN, FN classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_category(labels, preds, label_class, pred_class):\n",
    "    class_index = []\n",
    "    for label, pred in zip(labels, preds):\n",
    "        binary_pred = pred[1] > THRESHOLD\n",
    "        label = label[1]\n",
    "        if label == label_class and binary_pred == pred_class:\n",
    "            class_index.append(1)\n",
    "        else:\n",
    "            class_index.append(0)\n",
    "    return np.array(class_index).astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_index = isolate_category(labels, preds, 1, 1)\n",
    "tn_index = isolate_category(labels, preds, 0, 0)\n",
    "fp_index = isolate_category(labels, preds, 0, 1)\n",
    "fn_index = isolate_category(labels, preds, 1, 0)\n",
    "\n",
    "print(f\"True Positive Rate: {100 * sum(tp_index) / sum(labels[:,1] == 1):.1f}%\")\n",
    "print(f\"True Negative Rate: {100 * sum(tn_index) / sum(labels[:,1] == 0):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi = np.squeeze([(sample[7] - sample[3]) / (sample[7] + sample[3]) for sample in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3), dpi=100, facecolor=(1,1,1))\n",
    "plt.hist(ndvi, bins=100)\n",
    "plt.xlabel('NDVI Value')\n",
    "plt.title('NDVI of All Test Samples')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,3), dpi=100, facecolor=(1,1,1))\n",
    "plt.hist(ndvi, bins=300, cumulative=True, density=True)\n",
    "plt.xlabel('NDVI Value')\n",
    "plt.ylabel('Proporion below x-Value')\n",
    "plt.title('Cumulative NDVI Dist - All Test Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3), dpi=100, facecolor=(1,1,1))\n",
    "edges, bins, patches = plt.hist(ndvi[tn_index], bins=100, color='r', alpha=0.5, label='True Negative')\n",
    "plt.hist(ndvi[tp_index], bins=bins, alpha=0.5, label='True Positive')\n",
    "plt.xlabel('NDVI Value')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.1)\n",
    "plt.title('NDVI of all Test Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pixel_grid(preds, colors, title):\n",
    "    plt.figure(figsize=(20,20), dpi=50, facecolor=(1,1,1))\n",
    "    num_samples = np.min([len(preds), 100])\n",
    "    indices = np.random.choice(len(preds), num_samples)\n",
    "    for i in range(num_samples):\n",
    "        index = indices[i]\n",
    "        pred = preds[index]\n",
    "        rgb = colors[index] ** gamma\n",
    "        plt.subplot(10,10,i + 1)\n",
    "        plt.imshow(rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"{pred[1]:.2f}\")\n",
    "    plt.suptitle(title, size=40, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_thresh = -0.1\n",
    "index = ndvi < ndvi_thresh\n",
    "print(f\"{100 * sum(index) / len(index):.1f}% of test samples have an NDVI < {ndvi_thresh}\")\n",
    "plot_pixel_grid(np.stack((ndvi, ndvi), axis=-1)[index], \n",
    "                rgb[index], \n",
    "                'Colors of Randomly Selected Pixels with NDVI < ' + str(ndvi_thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_thresh = 0.5\n",
    "index = ndvi > ndvi_thresh\n",
    "print(f\"{100 * sum(index) / len(index):.1f}% of test samples have an NDVI > {ndvi_thresh}\")\n",
    "plot_pixel_grid(np.stack((ndvi, ndvi), axis=-1)[index], \n",
    "                rgb[index], \n",
    "                'Colors of Randomly Selected Pixels with NDVI > ' + str(ndvi_thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = -0.1\n",
    "upper_bound = 0.5\n",
    "index = np.logical_and(ndvi > lower_bound, ndvi < upper_bound)\n",
    "print(f\"{100 * sum(index) / len(index):.1f}% of test samples have an NDVI less than {lower_bound:.1f} and greater {upper_bound:.1f}\")\n",
    "plot_pixel_grid(np.stack((ndvi, ndvi), axis=-1)[index], \n",
    "                rgb[index], \n",
    "                f'Colors of Randomly Selected Pixels with NDVI between {lower_bound:.1f} and {upper_bound:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test images and observe impact of filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/training_sites'\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"tpa_patch_histories_toa.pkl\"), 'rb') as file:\n",
    "    positive_histories = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"negative_patch_histories_toa.pkl\"), 'rb') as file:\n",
    "    negative_histories = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize TPA Sites\n",
    "\n",
    "lower_bound = 0\n",
    "upper_bound = 0.4\n",
    "\n",
    "for site in positive_histories['2019-07-01']:\n",
    "    data = positive_histories['2019-06-01'][site]\n",
    "    img = normalize(np.stack((data['B4'], data['B3'], data['B2']), axis=-1)) ** gamma\n",
    "\n",
    "    img_ndvi = (data['B8'] - data['B4']) / (data['B8'] + data['B4'])\n",
    "    index = np.logical_or(img_ndvi < lower_bound, img_ndvi > upper_bound)\n",
    "    filtered_img = normalize(np.stack((data['B4'], data['B3'], data['B2']), axis=-1)) ** gamma\n",
    "    filtered_img[index] = 0\n",
    "\n",
    "    plt.figure(figsize=(6,3), dpi=100, facecolor=(1,1,1))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Data with no NDVI Filtering')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(filtered_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Data with NDVI between {lower_bound} and {upper_bound}')\n",
    "    plt.suptitle(site)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize non-TPA Sites\n",
    "for site in negative_histories[2]['2019-07-01']:\n",
    "    data = negative_histories[2]['2019-06-01'][site]\n",
    "    img = normalize(np.stack((data['B4'], data['B3'], data['B2']), axis=-1)) ** gamma\n",
    "    \n",
    "    img_ndvi = (data['B8'] - data['B4']) / (data['B8'] + data['B4'])\n",
    "    index = np.logical_or(img_ndvi < lower_bound, img_ndvi > upper_bound)\n",
    "    filtered_img = normalize(np.stack((data['B4'], data['B3'], data['B2']), axis=-1)) ** gamma\n",
    "    filtered_img[index] = 0\n",
    "\n",
    "    plt.figure(figsize=(6,3), dpi=100, facecolor=(1,1,1))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Data with no NDVI Filtering')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(filtered_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Data with NDVI between {lower_bound} and {upper_bound}')\n",
    "    plt.suptitle(site)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training dataset where NDVI is within a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '../data/training_sites'\n",
    "\n",
    "with open(os.path.join(train_data_dir, \"negative_data_toa.pkl\"), 'rb') as file:\n",
    "    x_negative = np.array(pickle.load(file))\n",
    "\n",
    "with open(os.path.join(train_data_dir, \"bootstrap_data_toa.pkl\"), 'rb') as file:\n",
    "    x_negative_bootstrap = np.array(pickle.load(file))\n",
    "    \n",
    "x_negative = np.concatenate((x_negative, x_negative_bootstrap))\n",
    "y_negative = np.zeros(len(x_negative))\n",
    "\n",
    "with open(os.path.join(train_data_dir, \"positive_data_toa.pkl\"), 'rb') as file:\n",
    "    x_positive = np.array(pickle.load(file))\n",
    "\n",
    "with open(os.path.join(train_data_dir, \"positive_data_test_toa.pkl\"), 'rb') as file:\n",
    "    x_positive_test = np.array(pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_train = (x_positive[:,7] - x_positive[:,3]) / (x_positive[:,7] + x_positive[:,3])\n",
    "ndvi_test = (x_positive_test[:,7] - x_positive_test[:,3]) / (x_positive_test[:,7] + x_positive_test[:,3])\n",
    "\n",
    "lower_bound = 0\n",
    "upper_bound = 0.4\n",
    "index_train = np.logical_and(ndvi_train > lower_bound, ndvi_train < upper_bound)\n",
    "index_test = np.logical_and(ndvi_test > lower_bound, ndvi_test < upper_bound)\n",
    "\n",
    "x_positive = x_positive[index_train]\n",
    "y_positive = np.ones(len(x_positive))\n",
    "x_positive_test = x_positive_test[index_test]\n",
    "y_positive_test = np.ones(len(x_positive_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((x_positive, x_negative))\n",
    "y = np.concatenate((y_positive, y_negative))\n",
    "\n",
    "x, y = shuffle(x, y, random_state=42)\n",
    "x = normalize(x)\n",
    "x_positive_test = normalize(x_positive_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter training data by NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3), dpi=100, facecolor=(1,1,1))\n",
    "edges, bins, patches = plt.hist(ndvi_train, bins=300, label='Original NDVI')\n",
    "plt.hist(ndvi_train[index_train], bins=bins, color='r', label='Clipped NDVI')\n",
    "plt.title('Distribution of NDVI in the Original Train Set')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3), dpi=100, facecolor=(1,1,1))\n",
    "plt.hist((x_filtered[:,7] - x_filtered[:,3]) / (x_filtered[:,7] + x_filtered[:,3]), bins=300)\n",
    "plt.title('Distribution of NDVI in Filtered Train Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "print(\"Num Train:\\t\\t\", len(x_train))\n",
    "print(\"Num Test:\\t\\t\", len(x_test))\n",
    "print(f\"Percent Negative Train:\\t {100 * sum(y_train == 0.0) / len(y_train):.1f}\")\n",
    "print(f\"Percent Negative Test:\\t {100 * sum(y_test == 0.0) / len(y_test):.1f}\")\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "x_positive_test = np.expand_dims(x_positive_test, -1)\n",
    "\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_positive_test = keras.utils.to_categorical(y_positive_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model on the filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = np.shape(x_train[0])\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv1D(16, kernel_size=(3), activation=\"relu\"),\n",
    "        #layers.MaxPooling2D(pool_size=(2)),\n",
    "        layers.Conv1D(32, kernel_size=(3), activation=\"relu\"),\n",
    "        #layers.MaxPooling2D(pool_size=(2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", \n",
    "              optimizer=\"adam\", \n",
    "              metrics=[keras.metrics.Recall(thresholds=(0.9), name='precision'), \n",
    "                       keras.metrics.Precision(thresholds=(0.9), name='recall'),\n",
    "                       keras.metrics.AUC(curve='PR', name='auc'),\n",
    "                       \"accuracy\"],\n",
    "              #loss_weights = sum(y_train) / len(y_train),\n",
    "              #weighted_metrics = ['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "negative_weight, positive_weight = class_weight.compute_class_weight('balanced', \n",
    "                                                           classes = np.unique(y_train),\n",
    "                                                           y = y_train[:,1])\n",
    "#positive_weight /= 3\n",
    "print(f\"Negative Weight: {negative_weight:.2f}\")\n",
    "print(f\"Positive Weight: {positive_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 30\n",
    "\n",
    "print(\"Num Train:\\t\\t\", len(x_train))\n",
    "print(\"Num Test:\\t\\t\", len(x_test))\n",
    "print(f\"Percent Negative Train:\\t {100 * sum(y_train[:,1] == 0.0) / len(y_train):.1f}\")\n",
    "print(f\"Percent Negative Test:\\t {100 * sum(y_test[:,1] == 0.0) / len(y_test):.1f}\")\n",
    "\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_data = (x_test, y_test),\n",
    "          #validation_split=0.1,\n",
    "          #class_weight = {0: negative_weight, 1: positive_weight}\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5), dpi=100, facecolor=(1,1,1))\n",
    "plt.plot(model.history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(model.history.history['val_accuracy'], c='r', label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Network Train and Val Accuracy - Weighted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5), dpi=100, facecolor=(1,1,1))\n",
    "plt.plot(model.history.history['precision'], label='precision')\n",
    "plt.plot(model.history.history['recall'], c='r', label='recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.title('Train and Val AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test[:,1], model.predict(x_test)[:,1] > 0.6, \n",
    "                            target_names=['No TPA', 'TPA']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test[:,1], model.predict(x_test)[:,1] > 0.9, target_names=['No Dump', 'Dump']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/65_mo_tpa_bootstrap_toa-12-20-2020.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Network Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 3\n",
    "ensemble = []\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "input_shape = np.shape(x_train[0])\n",
    "\n",
    "print(\"Num Train:\\t\\t\", len(x_train))\n",
    "print(\"Num Test:\\t\\t\", len(x_test))\n",
    "print(f\"Percent Negative Train:\\t {100 * sum(y_train[:,1] == 0.0) / len(y_train):.1f}\")\n",
    "print(f\"Percent Negative Test:\\t {100 * sum(y_test[:,1] == 0.0) / len(y_test):.1f}\")\n",
    "\n",
    "for i in range(num_models):\n",
    "    model = keras.Sequential([\n",
    "                keras.Input(shape=input_shape),\n",
    "                layers.Conv1D(16, kernel_size=(3), activation=\"relu\"),\n",
    "                #layers.MaxPooling2D(pool_size=(2)),\n",
    "                layers.Conv1D(32, kernel_size=(3), activation=\"relu\"),\n",
    "                #layers.MaxPooling2D(pool_size=(2)),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(32, activation=\"relu\"),\n",
    "                layers.Dense(32, activation=\"relu\"),\n",
    "                layers.Dense(32, activation=\"relu\"),\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Dense(num_classes, activation=\"softmax\")])\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \n",
    "                  optimizer=\"adam\", \n",
    "                  metrics=[keras.metrics.Recall(thresholds=(0.9), name='precision'), \n",
    "                           keras.metrics.Precision(thresholds=(0.9), name='recall'),\n",
    "                           keras.metrics.AUC(curve='PR', name='auc'),\n",
    "                           \"accuracy\"])\n",
    "\n",
    "    model.fit(x_train, \n",
    "              y_train, \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              validation_data = (x_test, y_test),\n",
    "              verbose = 2\n",
    "             )\n",
    "    \n",
    "    plt.figure(figsize=(8,5), dpi=100, facecolor=(1,1,1))\n",
    "    plt.plot(model.history.history['accuracy'], label='Train Acc')\n",
    "    plt.plot(model.history.history['val_accuracy'], c='r', label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Network Train and Val Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    ensemble.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../models/01-09-2020_ensemble'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "for index, model in enumerate(ensemble):\n",
    "    model.save(os.path.join(output_dir, 'model_' + str(index) + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel 2 band descriptions\n",
    "band_descriptions = {\n",
    "    'B1': 'Aerosols, 442nm',\n",
    "    'B2': 'Blue, 492nm',\n",
    "    'B3': 'Green, 559nm',\n",
    "    'B4': 'Red, 665nm',\n",
    "    'B5': 'Red Edge 1, 704nm',\n",
    "    'B6': 'Red Edge 2, 739nm',\n",
    "    'B7': 'Red Edge 3, 779nm',\n",
    "    'B8': 'NIR, 833nm',\n",
    "    'B8A': 'Red Edge 4, 864nm',\n",
    "    'B9': 'Water Vapor, 943nm',\n",
    "    'B11': 'SWIR 1, 1610nm',\n",
    "    'B12': 'SWIR 2, 2186nm'\n",
    "}\n",
    "\n",
    "band_wavelengths = [442, 492, 559, 665, 704, 739, 779, 833, 864, 943, 1610, 2186]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_leaf_nodes=512, max_depth=64)\n",
    "                                  #, class_weight='balanced')\n",
    "clf = clf.fit(np.squeeze(x_train), y_train[:,1])\n",
    "\n",
    "print(\"Accuracy:\", clf.score(np.squeeze(x_test), y_test[:,1]))\n",
    "print(\"Feature Importances:\")\n",
    "for band, importance in zip(band_descriptions, clf.feature_importances_):\n",
    "    print(f\"{band}: {importance:.3f}\")\n",
    "    \n",
    "plt.bar(range(len(band_descriptions)), clf.feature_importances_)\n",
    "plt.xticks(range(len(band_descriptions)), band_descriptions.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_importances = []\n",
    "\n",
    "for i in tqdm(range(20)):\n",
    "    clf = tree.DecisionTreeClassifier(max_leaf_nodes=512, max_depth=64, random_state=i)\n",
    "                                      #, class_weight='balanced')\n",
    "    clf = clf.fit(np.squeeze(x_train), y_train[:,1])\n",
    "    mean_importances.append(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5), dpi=100, facecolor=(1,1,1))\n",
    "plt.bar(range(len(band_descriptions)), np.mean(mean_importances, axis=0))\n",
    "plt.xticks(range(len(band_descriptions)), band_descriptions.keys())\n",
    "plt.title('Decision Tree Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test[:,1], clf.predict(np.squeeze(x_test)), target_names=['No Dump', 'Dump']))\n",
    "#plt.imshow(confusion_matrix(y_test[:,1], clf.predict(np.squeeze(x_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_tree(site_name, threshold):\n",
    "    with open(os.path.join(DATA_DIR, site_name + \"_0.03_patch.pkl\"), 'rb') as file:\n",
    "        test_image = pickle.load(file)\n",
    "\n",
    "    rgb_stack = []\n",
    "    preds_stack = []\n",
    "    threshold_stack = []\n",
    "\n",
    "    for month in tqdm(list(test_image.keys())):\n",
    "        test_pixel_vectors, width, height = get_pixel_vectors(test_image, month)\n",
    "        if width > 0:\n",
    "            test_pixel_vectors = normalize(test_pixel_vectors)\n",
    "\n",
    "            r = np.reshape(np.array(test_pixel_vectors)[:,3], (width, height))\n",
    "            g = np.reshape(np.array(test_pixel_vectors)[:,2], (width, height))\n",
    "            b = np.reshape(np.array(test_pixel_vectors)[:,1], (width, height))\n",
    "            rgb = np.moveaxis(np.stack((r,g,b)), 0, -1)\n",
    "            rgb_stack.append(rgb)\n",
    "\n",
    "            preds = clf.predict(test_pixel_vectors)\n",
    "            preds_img = np.reshape(preds, (width, height))\n",
    "            preds_stack.append(preds_img)\n",
    "\n",
    "            thresh_img = preds_img > threshold\n",
    "            threshold_stack.append(thresh_img)\n",
    "    \n",
    "    output_dir = './figures/tree_classification/12-08-2020'\n",
    "    if not os.path.exists(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "\n",
    "            \n",
    "    rgb_median = np.median(rgb_stack, axis=0)\n",
    "    preds_median = np.median(preds_stack, axis=0)\n",
    "    threshold_median = np.median(threshold_stack, axis=0)\n",
    "    \n",
    "    plt.figure(dpi=150, facecolor=(1,1,1), figsize=(15,5))\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(rgb_median ** gamma)\n",
    "    plt.title(f'{site_name} Median', size=8)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(preds_median, vmin=0, vmax=1, cmap='seismic')\n",
    "    plt.title('Classification Median', size=8)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(threshold_median, vmin=threshold, vmax=1, cmap='gray')\n",
    "    plt.title(f\"Positive Pixels Median: Threshold {threshold}\", size=8)\n",
    "    plt.axis('off')\n",
    "\n",
    "    title = f\"{site_name} Test Set - Median Values - Tree Classification - Threshold {threshold}\"\n",
    "    plt.suptitle(title, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, title + '.png'), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(dpi=200, facecolor=(1,1,1), figsize=(4,4))\n",
    "    ax.set_axis_off()\n",
    "    clipped_img = np.moveaxis([channel * (preds_median > 0) for channel in np.moveaxis(rgb_median, -1, 0)], 0, -1)\n",
    "    img = plt.imshow(clipped_img / (clipped_img.max()))\n",
    "    ax.set_title('Threshold 0')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    def animate(i):\n",
    "        i /= 100\n",
    "        clipped_img = np.moveaxis([channel * (preds_median > i) for channel in np.moveaxis(rgb_median, -1, 0)], 0, -1)\n",
    "        img.set_data(clipped_img / (clipped_img.max()))\n",
    "        #img.set_data((preds_stack > i) * 1)\n",
    "        ax.set_title(site_name + ' Threshold ' + str(i))\n",
    "        return img,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, frames=100, interval=60, blit=True, repeat_delay=500)\n",
    "    ani.save(os.path.join(output_dir, site_name + 'test_set_threshold_visualization' + '.mp4'))\n",
    "    plt.close()\n",
    "    \n",
    "    return rgb_median, preds_median, threshold_median\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data'\n",
    "site_names = ['bare_earth_4', 'city_7', 'tpa_babandem', 'tpa_bangli', 'tpa_biaung', 'tpa_mandung', 'tpa_jimbaran']\n",
    "threshold = 0.90\n",
    "\n",
    "for site_name in site_names:\n",
    "    rgb_median, preds_median, threshold_median = make_predictions_tree(site_name, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Network Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from matplotlib import animation\n",
    "\n",
    "sys.path.append('../')\n",
    "from scripts.get_s2_data_ee import get_pixel_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(site_name, threshold):\n",
    "    with open(os.path.join(DATA_DIR, site_name + \"_0.03_patch.pkl\"), 'rb') as file:\n",
    "        test_image = pickle.load(file)\n",
    "\n",
    "    rgb_stack = []\n",
    "    preds_stack = []\n",
    "    threshold_stack = []\n",
    "\n",
    "    for month in tqdm(list(test_image.keys())):\n",
    "        test_pixel_vectors, width, height = get_pixel_vectors(test_image, month)\n",
    "        if width > 0:\n",
    "            test_pixel_vectors = normalize(test_pixel_vectors)\n",
    "\n",
    "            r = np.reshape(np.array(test_pixel_vectors)[:,3], (width, height))\n",
    "            g = np.reshape(np.array(test_pixel_vectors)[:,2], (width, height))\n",
    "            b = np.reshape(np.array(test_pixel_vectors)[:,1], (width, height))\n",
    "            rgb = np.moveaxis(np.stack((r,g,b)), 0, -1)\n",
    "            rgb_stack.append(rgb)\n",
    "\n",
    "            preds = model.predict(np.expand_dims(test_pixel_vectors, axis=-1))\n",
    "            preds_img = np.reshape(preds, (width, height, 2))[:,:,1]\n",
    "            preds_stack.append(preds_img)\n",
    "\n",
    "            thresh_img = preds_img > threshold\n",
    "            threshold_stack.append(thresh_img)\n",
    "    \n",
    "    output_dir = './figures/neural_network/12-09-2020'\n",
    "    if not os.path.exists(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "\n",
    "            \n",
    "    rgb_median = np.median(rgb_stack, axis=0)\n",
    "    preds_median = np.sum(preds_stack, axis=0)\n",
    "    threshold_median = np.median(threshold_stack, axis=0)\n",
    "    \n",
    "    plt.figure(dpi=150, facecolor=(1,1,1), figsize=(15,5))\n",
    "    gamma = .85\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(rgb_median ** gamma)\n",
    "    plt.title(f'{site_name} Median', size=8)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(preds_median, vmin=0, vmax=1, cmap='seismic')\n",
    "    plt.title('Classification Median', size=8)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(threshold_median, vmin=threshold, vmax=1, cmap='gray')\n",
    "    plt.title(f\"Positive Pixels Median: Threshold {threshold}\", size=8)\n",
    "    plt.axis('off')\n",
    "\n",
    "    title = f\"{site_name} Test Set - Median Values - Neural Network Classification - Threshold {threshold}\"\n",
    "    plt.suptitle(title, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(os.path.join(output_dir, title + '.png'), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(dpi=200, facecolor=(1,1,1), figsize=(4,4))\n",
    "    ax.set_axis_off()\n",
    "    clipped_img = np.moveaxis([channel * (preds_median >= 0) for channel in np.moveaxis(rgb_median, -1, 0)], 0, -1)\n",
    "    img = plt.imshow(clipped_img / (clipped_img.max()))\n",
    "    ax.set_title('Threshold 0')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    def animate(i):\n",
    "        i /= 100\n",
    "        clipped_img = np.moveaxis([channel * (preds_median >= i) for channel in np.moveaxis(rgb_median, -1, 0)], 0, -1)\n",
    "        img.set_data(clipped_img / (clipped_img.max()))\n",
    "        #img.set_data((preds_stack > i) * 1)\n",
    "        ax.set_title(site_name + ' Threshold ' + str(i))\n",
    "        return img,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, frames=100, interval=60, blit=True, repeat_delay=500)\n",
    "    #ani.save(os.path.join(output_dir, site_name + 'test_set_threshold_visualization' + '.mp4'))\n",
    "    plt.close()\n",
    "    \n",
    "    return rgb_stack, preds_stack, threshold_stack\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_comparison(site_name, threshold):\n",
    "    with open(os.path.join(DATA_DIR, site_name + \"_0.03_patch.pkl\"), 'rb') as file:\n",
    "        test_image = pickle.load(file)\n",
    "\n",
    "    rgb_stack = []\n",
    "    preds_stack = []\n",
    "    threshold_stack = []\n",
    "\n",
    "    for month in tqdm(list(test_image.keys())):\n",
    "        test_pixel_vectors, width, height = get_pixel_vectors(test_image, month)\n",
    "        if width > 0:\n",
    "            test_pixel_vectors = normalize(test_pixel_vectors)\n",
    "\n",
    "            r = np.reshape(np.array(test_pixel_vectors)[:,3], (width, height))\n",
    "            g = np.reshape(np.array(test_pixel_vectors)[:,2], (width, height))\n",
    "            b = np.reshape(np.array(test_pixel_vectors)[:,1], (width, height))\n",
    "            rgb = np.moveaxis(np.stack((r,g,b)), 0, -1)\n",
    "            rgb_stack.append(rgb)\n",
    "\n",
    "            preds = model.predict(np.expand_dims(test_pixel_vectors, axis=-1))\n",
    "            preds_img = np.reshape(preds, (width, height, 2))[:,:,1]\n",
    "            preds_stack.append(preds_img)\n",
    "\n",
    "            thresh_img = preds_img > threshold\n",
    "            threshold_stack.append(thresh_img)\n",
    "    \n",
    "    output_dir = './figures/neural_network/12-09-2020'\n",
    "    if not os.path.exists(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "\n",
    "            \n",
    "    rgb_median = np.median(rgb_stack, axis=0)\n",
    "    preds_median = np.median(preds_stack, axis=0)\n",
    "    threshold_median = np.median(threshold_stack, axis=0)\n",
    "    \n",
    "    plt.figure(dpi=150, facecolor=(1,1,1), figsize=(15,5))\n",
    "    gamma = .85\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(rgb_median ** gamma)\n",
    "    plt.title(f'{site_name} Median', size=8)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(np.mean(preds_stack, axis=0), vmin=.6, vmax=1, cmap='seismic')\n",
    "    plt.axis('off')\n",
    "    #plt.colorbar()\n",
    "    plt.title('mean')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(np.median(preds_stack, axis=0), vmin=.6, vmax=1, cmap='seismic')\n",
    "    plt.axis('off')\n",
    "    #plt.colorbar()\n",
    "    plt.title('median')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(dpi=200, facecolor=(1,1,1), figsize=(4,4))\n",
    "    ax.set_axis_off()\n",
    "    clipped_img = np.moveaxis([channel * (preds_median >= 0) for channel in np.moveaxis(rgb_median, -1, 0)], 0, -1)\n",
    "    img = plt.imshow(clipped_img / (clipped_img.max()))\n",
    "    ax.set_title('Threshold 0')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    def animate(i):\n",
    "        i /= 100\n",
    "        clipped_img = np.moveaxis([channel * (preds_median >= i) for channel in np.moveaxis(rgb_median, -1, 0)], 0, -1)\n",
    "        img.set_data(clipped_img / (clipped_img.max()))\n",
    "        #img.set_data((preds_stack > i) * 1)\n",
    "        ax.set_title(site_name + ' Threshold ' + str(i))\n",
    "        return img,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, frames=100, interval=60, blit=True, repeat_delay=500)\n",
    "    #ani.save(os.path.join(output_dir, site_name + 'test_set_threshold_visualization' + '.mp4'))\n",
    "    plt.close()\n",
    "    \n",
    "    return rgb_stack, preds_stack, threshold_stack\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../models/model_filtered-12-07-2020.h5')\n",
    "DATA_DIR = '../data'\n",
    "site_names = ['bare_earth_4', 'tpa_babandem', 'city_7', 'tpa_bangli', 'tpa_biaung', 'tpa_mandung', 'tpa_jimbaran']\n",
    "threshold = 0.90\n",
    "\n",
    "for site_name in site_names:\n",
    "    rgb_median, preds_median, threshold_median = make_predictions_comparison(site_name, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6), dpi=150)\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(np.median(rgb_median, axis=0))\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(np.mean(preds_median, axis=0), vmin=.6, vmax=1, cmap='seismic')\n",
    "plt.axis('off')\n",
    "#plt.colorbar()\n",
    "plt.title('mean')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(np.median(preds_median, axis=0), vmin=.6, vmax=1, cmap='seismic')\n",
    "plt.axis('off')\n",
    "#plt.colorbar()\n",
    "plt.title('median')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6), dpi=150)\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(rgb_median, cmap='seismic')\n",
    "plt.title('Classification Median', size=8)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(preds_median, vmin=6, vmax=9, cmap='seismic')\n",
    "plt.title('Classification Median', size=8)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_median.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_time_series(patch_histories, site_name, threshold, model):\n",
    "    rgb_stack = []\n",
    "    preds_stack = []\n",
    "    threshold_stack = []\n",
    "    \n",
    "    for date in dates:\n",
    "        rgb = np.stack((tpa_histories[date][site_name]['B4'],\n",
    "                        tpa_histories[date][site_name]['B3'],\n",
    "                        tpa_histories[date][site_name]['B2']), axis=-1)\n",
    "        if len(rgb) > 0:\n",
    "            rgb_stack.append(rgb / 3000)\n",
    "        \n",
    "        width, height = rgb.shape[:2]\n",
    "        pixel_vectors = []\n",
    "        for i in range(width):\n",
    "            for j in range(height):\n",
    "                pixel_vector = []\n",
    "                for band in band_descriptions:\n",
    "                    pixel_vector.append(tpa_histories[date][site_name][band][i][j])\n",
    "                pixel_vectors.append(pixel_vector)\n",
    "        \n",
    "        pixel_vectors = normalize(pixel_vectors)\n",
    "        if len(pixel_vectors) > 0:\n",
    "            preds = model.predict(np.expand_dims(pixel_vectors, axis=-1))\n",
    "            preds_img = np.reshape(preds, (width, height, 2))[:,:,1]\n",
    "            preds_stack.append(preds_img)\n",
    "            \n",
    "    return np.array(rgb_stack), np.array(preds_stack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPA Time Series Predictions - TOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "base_path = '/Users/ckruse/Documents/earthrise/plastics'\n",
    "with open(os.path.join(base_path, 'data', 'training_sites', 'tpa_patch_histories_toa.pkl'), 'rb') as file:\n",
    "    tpa_histories = pickle.load(file)\n",
    "file.close()\n",
    "dates = list(tpa_histories.keys())\n",
    "sites = list(tpa_histories[dates[0]].keys())\n",
    "bands = list(tpa_histories[dates[0]][sites[0]].keys())\n",
    "print(len(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../models/model_65_month_filtered_toa-12-09-2020.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(rgb_stack, preds_stack) = predict_time_series(tpa_histories, sites[6], 0.9, model)\n",
    "for rgb, pred in zip(rgb_stack, preds_stack):\n",
    "    if np.median(rgb) > 0:\n",
    "        plt.figure(dpi=150)\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title('RGB')\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(pred, vmin=0, vmax=1, cmap='seismic')\n",
    "        plt.axis('off')\n",
    "        plt.title('Prediction')\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        rgb[:,:,0] += pred\n",
    "        plt.imshow(rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title('Composite')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPA Time Series Prediction - SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "base_path = '/Users/ckruse/Documents/earthrise/plastics'\n",
    "with open(os.path.join(base_path, 'data', 'training_sites', 'tpa_patch_histories.pkl'), 'rb') as file:\n",
    "    tpa_histories = pickle.load(file)\n",
    "file.close()\n",
    "dates = list(tpa_histories.keys())\n",
    "sites = list(tpa_histories[dates[0]].keys())\n",
    "bands = list(tpa_histories[dates[0]][sites[0]].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../models/model_filtered-12-07-2020.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rgb_stack, preds_stack) = predict_time_series(tpa_histories, sites[6], 0.9, model)\n",
    "for rgb, pred in zip(rgb_stack, preds_stack):\n",
    "    if np.median(rgb) > 0:\n",
    "        plt.figure(dpi=150)\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title('RGB')\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(pred, vmin=0, vmax=1, cmap='seismic')\n",
    "        plt.axis('off')\n",
    "        plt.title('Prediction')\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        rgb[:,:,0] += pred\n",
    "        plt.imshow(rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title('Composite')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in sites:\n",
    "    (rgb_stack, preds_stack) = predict_time_series(tpa_histories, site, 0.75, model)\n",
    "    fig, ax = plt.subplots(dpi=100, facecolor=(1,1,1))\n",
    "    ax.set_axis_off()\n",
    "    images = []\n",
    "    for rgb, pred in zip(rgb_stack, preds_stack):\n",
    "        if np.median(rgb) > 0:\n",
    "            ax.set_title('TOA ' + site)\n",
    "            overlay = np.copy(rgb)\n",
    "            overlay[:,:,0] += pred\n",
    "            divider = np.ones((pred.shape[0], 1, 3))\n",
    "            pred[pred < 0.9] = 0\n",
    "            pred[pred >= 0.9] = 1\n",
    "            pred = np.stack((pred, pred, pred), axis=-1)\n",
    "            combination = np.concatenate((rgb, divider, overlay, divider, pred), axis=1)\n",
    "            im = plt.imshow(combination, animated=True)\n",
    "            \n",
    "            images.append([im])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    ani = animation.ArtistAnimation(fig, images, interval=200, blit=True, repeat_delay=500)\n",
    "    ani.save(os.path.join('figures', 'videos', site + ' TOA 12-09 preds.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
