{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Blob Detection on Pixel Heatmap to Identify Candidate Sites\n",
    "Note: This is only working on inputs with EPSG CRS 4326. I may need to make it more general in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import geojson\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rs\n",
    "from rasterio.windows import Window\n",
    "from rasterio import warp\n",
    "from skimage.feature import blob_doh\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixels_to_coords(x, y, src_img):\n",
    "    lon, lat = warp.transform(src_img.crs, rs.crs.CRS.from_epsg(4326), [src_img.xy(x, y)[0]], [src_img.xy(x, y)[1]])\n",
    "    return lon[0], lat[0]\n",
    "\n",
    "def coords_to_pixels(lon, lat, src_img):\n",
    "    x, y = warp.transform(rs.crs.CRS.from_epsg(4326), src_img.crs, [lon],  [lat])\n",
    "    pixel_y, pixel_x = src_img.index(x, y)\n",
    "    return pixel_x[0], pixel_y[0]\n",
    "\n",
    "def merge_similar_sites(candidate_sites, search_radius=0.01):\n",
    "    \"\"\"\n",
    "    This process iteratively moves points closer together by taking the mean position of all\n",
    "    matched points. It then searches the KD tree using the unique clusters in these new points.\n",
    "    The algorithm stops once the number of unique sites is the same as in the previous round.\n",
    "    \n",
    "    search_radius is given in degrees\n",
    "    \"\"\"\n",
    "    #coords = np.array([[lon, lat] for lon, lat in zip(candidate_site_df['lon'], candidate_site_df['lat'])])\n",
    "    coords = np.array(candidate_sites)\n",
    "    \n",
    "    # Create a KD tree for efficient lookup of points within a radius\n",
    "    tree = KDTree(coords, leaf_size=2)\n",
    "    \n",
    "    # Initialize a mean_coords array for the search\n",
    "    mean_coords = []\n",
    "    for elem in tree.query_radius(coords, search_radius):\n",
    "        mean_coords.append(np.mean(coords[elem], axis=0))\n",
    "    mean_coords = np.array(mean_coords)\n",
    "    \n",
    "    num_coords = len(mean_coords)\n",
    "    while True:\n",
    "        search = tree.query_radius(mean_coords, search_radius)\n",
    "        uniques = [list(x) for x in set(tuple(elem) for elem in search)]\n",
    "        mean_coords = []\n",
    "        for elem in uniques:\n",
    "            mean_coords.append(np.mean(coords[elem], axis=0))\n",
    "        if len(mean_coords) == num_coords:\n",
    "            print(len(mean_coords), \"unique sites detected\")\n",
    "            mean_coords = np.array(mean_coords)\n",
    "            break\n",
    "        num_coords = len(mean_coords)\n",
    "        \n",
    "    unique_sites = pd.DataFrame(mean_coords, columns=['lon', 'lat'])\n",
    "    unique_sites['name'] = [f\"{name}_{i+1}\" for i in range(len(unique_sites))]\n",
    "    plt.scatter(coords[:,0], coords[:,1], s=5, label='Original')\n",
    "    plt.scatter(mean_coords[:,0], mean_coords[:,1], s=3, c='r', label='Unique')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return unique_sites\n",
    "\n",
    "def detect_candidates(source_img, name, pred_threshold=0.75, min_sigma=3.5, max_sigma=100, area_threshold=0.0025, window_size=5000):\n",
    "    \"\"\"\n",
    "    Identify candidates using blob detection on the heatmap.\n",
    "    prediction_threshold masks any prediction below a 0-1 threshold.\n",
    "    min_sigma and area_threshold control the size sensitivity of the blob detection.\n",
    "    Keep min_sigma low to detect smaller blobs\n",
    "    area_threshold establishes a lower bound on candidate blob size. Reduce to detect smaller blobs\n",
    "    \"\"\"\n",
    "    candidate_sites = []\n",
    "    max_val = source.read(1).max()\n",
    "    for x in range(0, source.shape[0], window_size):\n",
    "        for y in range(0, source.shape[1], window_size):\n",
    "            print(f\"Processing row {(x // window_size) + 1} of {int(source.shape[0] / window_size) + 1}, column {(y // window_size) + 1} of {int(source.shape[1] / window_size) + 1}\")\n",
    "            # Set min and max to analyze a subset of the image\n",
    "            window = Window.from_slices((x,x + window_size), (y, y + window_size))\n",
    "            window_median = (source.read(1, window=window) / max_val).astype('float')\n",
    "            # mask predictions below a threshold\n",
    "            mask = np.ma.masked_where(window_median < pred_threshold, window_median).mask\n",
    "            window_median[mask] = 0\n",
    "\n",
    "            blobs = blob_doh(window_median, min_sigma=min_sigma, max_sigma=max_sigma, threshold=area_threshold)\n",
    "            print(len(blobs), \"candidates detected in window\")\n",
    "            \n",
    "            overlap_threshold = 0.01\n",
    "            transform = source.window_transform(window)\n",
    "            for candidate in blobs:\n",
    "                lon, lat = (transform * [candidate[1], candidate[0]])\n",
    "                # Size doesn't mean anything at the moment. Should look into this later\n",
    "                #size = candidate[2]\n",
    "                candidate_sites.append([lon, lat])\n",
    "    \n",
    "    print(len(candidate_sites), \"candidate sites detected in total\")\n",
    "    \n",
    "    candidate_site_df = merge_similar_sites(candidate_sites, search_radius=0.01)\n",
    "    \n",
    "    # Write candidates to CSV and GeoJSON\n",
    "    file_path = f\"../data/model_outputs/candidate_sites/{name}_candidates_pred-thresh_{pred_threshold}_min-sigma_{min_sigma}_area-thresh_{area_threshold}\"\n",
    "    candidate_site_df.to_csv(file_path + '.csv', index = False)\n",
    "    print(\"Candidate sites written to\", file_path)\n",
    "    \n",
    "    geojson_features = []\n",
    "    for i, site in enumerate(unique_sites):\n",
    "        geojson_features.append(geojson.Feature(geometry = geojson.Point(site),\n",
    "                                                properties={'name': f\"{name}_{i+1}\"}))\n",
    "    feature_collection = geojson.FeatureCollection(geojson_features)\n",
    "    with open(file_path + '.geojson', 'w') as f:\n",
    "        f.write(geojson.dumps(feature_collection))\n",
    "    \n",
    "    return candidate_site_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Bali_v1.1.5_2019-2020'\n",
    "source = rs.open(f'../data/model_outputs/heatmaps/{name}.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values are tuned for Bali detections and are still a work in progress\n",
    "pred_threshold = 0.80\n",
    "min_sigma=5\n",
    "max_sigma=100\n",
    "area_threshold=0.0025\n",
    "\n",
    "candidate_site_df = detect_candidates(source, \n",
    "                                      name, \n",
    "                                      pred_threshold=pred_threshold, \n",
    "                                      min_sigma=min_sigma, \n",
    "                                      max_sigma=max_sigma, \n",
    "                                      area_threshold=area_threshold, \n",
    "                                      window_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepler_config = {\n",
    "  \"version\": \"v1\",\n",
    "  \"config\": {\n",
    "    \"visState\": {\n",
    "      \"layers\": [\n",
    "        {\n",
    "          \"id\": \"iik903a\",\n",
    "          \"type\": \"point\",\n",
    "          \"config\": {\n",
    "            \"dataId\": \"Candidate Sites\",\n",
    "            \"label\": \"Point\",\n",
    "            \"color\": [\n",
    "              218,\n",
    "              0,\n",
    "              0\n",
    "            ],\n",
    "            \"columns\": {\n",
    "              \"lat\": \"lat\",\n",
    "              \"lng\": \"lon\",\n",
    "              \"altitude\": None\n",
    "            },\n",
    "            \"isVisible\": True,\n",
    "            \"visConfig\": {\n",
    "              \"radius\": 20,\n",
    "              \"fixedRadius\": False,\n",
    "              \"opacity\": 0.99,\n",
    "              \"outline\": True,\n",
    "              \"thickness\": 3,\n",
    "              \"strokeColor\": [\n",
    "                210,\n",
    "                0,\n",
    "                0\n",
    "              ],\n",
    "              \"filled\": False\n",
    "            },\n",
    "          },\n",
    "        }\n",
    "      ],\n",
    "    },\n",
    "    \"mapStyle\": {\n",
    "      \"styleType\": \"satellite\",\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot blob locations on a satellite base image\n",
    "from keplergl import KeplerGl\n",
    "\n",
    "candidate_map = KeplerGl(height=800, config=kepler_config)\n",
    "candidate_map.add_data(data=candidate_site_df, name='Candidate Sites')\n",
    "candidate_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
