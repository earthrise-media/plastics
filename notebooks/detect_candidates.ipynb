{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Blob Detection on Pixel Heatmap to Identify Candidate Sites\n",
    "Note: This is only working on inputs with EPSG CRS 4326. I may need to make it more general in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from keplergl import KeplerGl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import rasterio as rs\n",
    "from rasterio.windows import Window\n",
    "from skimage.feature import blob_doh\n",
    "from skimage.feature.peak import peak_local_max\n",
    "from sklearn.neighbors import KDTree\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.contrib.concurrent import process_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_similar_sites(candidate_sites, search_radius=0.0025):\n",
    "    \"\"\"\n",
    "    This process iteratively moves points closer together by taking the mean position of all\n",
    "    matched points. It then searches the KD tree using the unique clusters in these new points.\n",
    "    The algorithm stops once the number of unique sites is the same as in the previous round.\n",
    "    \n",
    "    search_radius is given in degrees\n",
    "    \"\"\"\n",
    "    coords = np.array(candidate_sites)\n",
    "    \n",
    "    # Create a KD tree for efficient lookup of points within a radius\n",
    "    tree = KDTree(coords, leaf_size=2)\n",
    "    \n",
    "    # Initialize a mean_coords array for the search\n",
    "    mean_coords = []\n",
    "    for elem in tree.query_radius(coords, search_radius):\n",
    "        mean_coords.append(np.mean(coords[elem], axis=0))\n",
    "    mean_coords = np.array(mean_coords)\n",
    "    \n",
    "    num_coords = len(mean_coords)\n",
    "    while True:\n",
    "        search = tree.query_radius(mean_coords, search_radius)\n",
    "        uniques = [list(x) for x in set(tuple(elem) for elem in search)]\n",
    "        mean_coords = []\n",
    "        for elem in uniques:\n",
    "            mean_coords.append(np.mean(coords[elem], axis=0))\n",
    "        if len(mean_coords) == num_coords:\n",
    "            print(len(mean_coords), \"unique sites detected\")\n",
    "            mean_coords = np.array(mean_coords)\n",
    "            break\n",
    "        num_coords = len(mean_coords)\n",
    "        \n",
    "    unique_sites = gpd.GeoDataFrame(mean_coords, columns=['lon', 'lat'], geometry=gpd.points_from_xy(*mean_coords.T))\n",
    "    unique_sites['name'] = [f\"{name}_{i+1}\" for i in unique_sites.index]\n",
    "    plt.figure(figsize=(10,10), dpi=150, facecolor=(1,1,1))\n",
    "    plt.scatter(coords[:,0], coords[:,1], s=5, label='Original')\n",
    "    plt.scatter(mean_coords[:,0], mean_coords[:,1], s=3, c='r', label='Unique')\n",
    "    plt.axis('equal')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return unique_sites\n",
    "\n",
    "def detect_blobs(source, name, pred_threshold=0.75, min_sigma=3.5, max_sigma=100, area_threshold=0.0025, window_size=5000, save=True):\n",
    "    \"\"\"\n",
    "    Identify candidates using blob detection on the heatmap.\n",
    "    prediction_threshold masks any prediction below a 0-1 threshold.\n",
    "    min_sigma and area_threshold control the size sensitivity of the blob detection.\n",
    "    Keep min_sigma low to detect smaller blobs\n",
    "    area_threshold establishes a lower bound on candidate blob size. Reduce to detect smaller blobs\n",
    "    \"\"\"\n",
    "    candidate_sites = []\n",
    "    max_val = source.read(1).max()\n",
    "    for x in range(0, source.shape[0], window_size):\n",
    "        for y in range(0, source.shape[1], window_size):\n",
    "            print(f\"Processing row {(x // window_size) + 1} of {int(source.shape[0] / window_size) + 1}, column {(y // window_size) + 1} of {int(source.shape[1] / window_size) + 1}\")\n",
    "            # Set min and max to analyze a subset of the image\n",
    "            window = Window.from_slices((x,x + window_size), (y, y + window_size))\n",
    "            window_median = (source.read(1, window=window) / max_val).astype('float')\n",
    "            # mask predictions below a threshold\n",
    "            mask = np.ma.masked_where(window_median < pred_threshold, window_median).mask\n",
    "            window_median[mask] = 0\n",
    "\n",
    "            blobs = blob_doh(window_median, min_sigma=min_sigma, max_sigma=max_sigma, threshold=area_threshold)\n",
    "            print(len(blobs), \"candidates detected in window\")\n",
    "            \n",
    "            overlap_threshold = 0.01\n",
    "            transform = source.window_transform(window)\n",
    "            for candidate in blobs:\n",
    "                lon, lat = (transform * [candidate[1], candidate[0]])\n",
    "                # Size doesn't mean anything at the moment. Should look into this later\n",
    "                #size = candidate[2]\n",
    "                candidate_sites.append([lon, lat])\n",
    "    \n",
    "    print(len(candidate_sites), \"candidate sites detected in total\")\n",
    "    \n",
    "    candidate_gdf = merge_similar_sites(candidate_sites, search_radius=0.01)\n",
    "    display(candidate_gdf)\n",
    "    \n",
    "    if save:\n",
    "        file_path = f\"../data/model_outputs/candidate_sites/{name}_blobs_thresh_{pred_threshold}_min-sigma_{min_sigma}_area-thresh_{area_threshold}\"\n",
    "        # candidate_gdf.loc[:, ['lon', 'lat', 'name']].to_csv(file_path + '.csv', index=False)\n",
    "        candidate_gdf.to_file(file_path + '.geojson', driver='GeoJSON')\n",
    "    \n",
    "    return candidate_gdf\n",
    "\n",
    "def blob_detect(file, pred_threshold=0.75, min_sigma=3.5, max_sigma=100, area_threshold=0.0025, save=True):\n",
    "    \"\"\"\n",
    "    Identify candidates using blob detection on the heatmap.\n",
    "    prediction_threshold masks any prediction below a 0-1 threshold.\n",
    "    min_sigma and area_threshold control the size sensitivity of the blob detection.\n",
    "    Keep min_sigma low to detect smaller blobs\n",
    "    area_threshold establishes a lower bound on candidate blob size. Reduce to detect smaller blobs\n",
    "    \"\"\"\n",
    "    \n",
    "    source = rs.open(file)\n",
    "    data = source.read(1).astype('float')\n",
    "    # mask predictions below a threshold\n",
    "    mask = np.ma.masked_where(data < pred_threshold, data).mask\n",
    "    data[mask] = 0\n",
    "    \n",
    "    candidate_sites = []\n",
    "\n",
    "    blobs = blob_doh(data, min_sigma=min_sigma, max_sigma=max_sigma, threshold=area_threshold)\n",
    "    if len(blobs) > 0:\n",
    "        print(len(blobs), \"candidate(s) detected in window\")\n",
    "        transform = source.transform\n",
    "        for candidate in blobs:\n",
    "            lon, lat = (transform * [candidate[1], candidate[0]])\n",
    "            # Size doesn't mean anything at the moment. Should look into this later\n",
    "            #size = candidate[2]\n",
    "            candidate_sites.append([lon, lat])\n",
    "    \n",
    "        return candidate_sites\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def detect_blobs_tiled(source_dir, pred_threshold=0.75, min_sigma=3.5, max_sigma=100, area_threshold=0.0025, save=True, merge_radius=0.005):\n",
    "    \"\"\"\n",
    "    Identify candidates using blob detection on the heatmap.\n",
    "    prediction_threshold masks any prediction below a 0-1 threshold.\n",
    "    min_sigma and area_threshold control the size sensitivity of the blob detection.\n",
    "    Keep min_sigma low to detect smaller blobs\n",
    "    area_threshold establishes a lower bound on candidate blob size. Reduce to detect smaller blobs\n",
    "    \"\"\"\n",
    "    files = os.listdir(source_dir)\n",
    "    file_paths = [os.path.join(source_dir, file) for file in files]\n",
    "    \n",
    "    blob_detect_partial = functools.partial(blob_detect, \n",
    "                                            pred_threshold=pred_threshold, \n",
    "                                            min_sigma=min_sigma, \n",
    "                                            max_sigma=max_sigma, \n",
    "                                            area_threshold=area_threshold, \n",
    "                                            save=save)\n",
    "    \n",
    "    site_list = process_map(blob_detect_partial, file_paths)\n",
    "    \n",
    "    candidate_sites = []\n",
    "    for tile_sites in site_list:\n",
    "        if tile_sites:\n",
    "            for site in tile_sites:\n",
    "                candidate_sites.append(site)\n",
    "    print(len(candidate_sites), \"sites detected overall\")\n",
    "    candidate_gdf = merge_similar_sites(candidate_sites, search_radius=merge_radius)\n",
    "    \n",
    "    print(len(candidate_sites) - len(candidate_gdf), \"sites merged\")\n",
    "    if save:\n",
    "        directory = f\"../data/model_outputs/candidate_sites/{MODEL_NAME}\"\n",
    "        file_name = f\"{name}_blobs_thresh_{pred_threshold}_min-sigma_{min_sigma}_area-thresh_{area_threshold}\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        candidate_gdf.to_file(file_path + '.geojson', driver='GeoJSON')\\\n",
    "    \n",
    "    return candidate_gdf\n",
    "\n",
    "\n",
    "\n",
    "def detect_peaks(source, name, threshold_abs=0.85, min_distance=100, window_size=5000, save=True):\n",
    "    \"\"\"\n",
    "    Identify candidates using heatmap peak detection.\n",
    "    Inputs:\n",
    "      source: rasterio geotiff object\n",
    "      name: file name\n",
    "      threshold_abs: threshold for minimum prediction value\n",
    "      min_distance: candidates within this distance will be merged by default. Distance in pixel space\n",
    "      window_size: chunk the image into windows to reduce memory load\n",
    "      save: boolean to write outputs to disk\n",
    "    \"\"\"\n",
    "    candidate_peaks = []\n",
    "    for x in range(0, source.shape[0], window_size):\n",
    "        for y in range(0, source.shape[1], window_size):\n",
    "            window = Window.from_slices((x,x + window_size), (y, y + window_size))\n",
    "            transform = source.window_transform(window)\n",
    "            subset = source.read(1, window=window)\n",
    "            peaks = peak_local_max(subset, threshold_abs=threshold_abs, min_distance=min_distance)\n",
    "            for candidate in peaks:\n",
    "                lon, lat = (transform * [candidate[1], candidate[0]])\n",
    "                candidate_peaks.append([lon, lat])\n",
    "    print(len(candidate_peaks), \"peaks detected\")\n",
    "    candidate_peaks = np.array(candidate_peaks)\n",
    "    \n",
    "    candidate_gdf = gpd.GeoDataFrame(candidate_peaks, columns=['lon', 'lat'], \n",
    "                                     geometry=gpd.points_from_xy(*candidate_peaks.T))\n",
    "    candidate_gdf['name'] = [f\"{name}_{i+1}\" for i in candidate_gdf.index]\n",
    "    \n",
    "    if save:\n",
    "        file_path = f\"../data/model_outputs/candidate_sites/{name}_peaks_thresh_{threshold_abs}_min_dist_{min_distance}\"\n",
    "        # candidate_gdf.loc[:, ['lon', 'lat', 'name']].to_csv(file_path + '.csv', index=False)\n",
    "        candidate_gdf.to_file(file_path + '.geojson', driver='GeoJSON')\n",
    "    \n",
    "    return candidate_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepler_config = {\n",
    "  \"version\": \"v1\",\n",
    "  \"config\": {\n",
    "    \"visState\": {\n",
    "      \"layers\": [\n",
    "        {\n",
    "          \"id\": \"iik903a\",\n",
    "          \"type\": \"point\",\n",
    "          \"config\": {\n",
    "            \"dataId\": \"Candidate Peaks\",\n",
    "            \"label\": \"Point\",\n",
    "            \"color\": [\n",
    "              218,\n",
    "              0,\n",
    "              0\n",
    "            ],\n",
    "            \"columns\": {\n",
    "              \"lat\": \"lat\",\n",
    "              \"lng\": \"lon\",\n",
    "              \"altitude\": None\n",
    "            },\n",
    "            \"isVisible\": True,\n",
    "            \"visConfig\": {\n",
    "              \"radius\": 20,\n",
    "              \"fixedRadius\": False,\n",
    "              \"opacity\": 0.99,\n",
    "              \"outline\": True,\n",
    "              \"thickness\": 3,\n",
    "              \"strokeColor\": [\n",
    "                210,\n",
    "                0,\n",
    "                0\n",
    "              ],\n",
    "              \"filled\": False\n",
    "            },\n",
    "          },\n",
    "        },\n",
    "        {\n",
    "          \"id\": \"kyoc7uj\",\n",
    "          \"type\": \"point\",\n",
    "          \"config\": {\n",
    "            \"dataId\": \"Candidate Blobs\",\n",
    "            \"label\": \"Point\",\n",
    "            \"color\": [\n",
    "              246,\n",
    "              218,\n",
    "              0\n",
    "            ],\n",
    "            \"columns\": {\n",
    "              \"lat\": \"lat\",\n",
    "              \"lng\": \"lon\",\n",
    "              \"altitude\": None\n",
    "            },\n",
    "            \"isVisible\": True,\n",
    "            \"visConfig\": {\n",
    "              \"radius\": 20,\n",
    "              \"fixedRadius\": False,\n",
    "              \"opacity\": 0.99,\n",
    "              \"outline\": True,\n",
    "              \"thickness\": 3,\n",
    "              \"strokeColor\": [\n",
    "                246,\n",
    "                218,\n",
    "                0\n",
    "              ],\n",
    "              \"filled\": False\n",
    "            },\n",
    "          },\n",
    "        }\n",
    "      ],\n",
    "    },\n",
    "    \"mapStyle\": {\n",
    "      \"styleType\": \"satellite\",\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "candidate_map = KeplerGl(height=800, config=kepler_config)\n",
    "candidate_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'v0.0.7'\n",
    "name = 'east_sunda_islands_v0.0.7_2019-01-01_2021-06-01mosaic-median'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Detect candidates from directory of tiles. Multiprocessed\n",
    "source_dir = f'../data/model_outputs/heatmaps/{MODEL_NAME}/{name}'\n",
    "blobs = detect_blobs_tiled(source_dir, \n",
    "                           pred_threshold=0.9, \n",
    "                           min_sigma=3.5, \n",
    "                           max_sigma=100, \n",
    "                           area_threshold=0.0025, \n",
    "                           save=True,\n",
    "                           merge_radius=0.005\n",
    "                          )\n",
    "candidate_map.add_data(data=blobs.copy(), name='Candidate Blobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = rs.open(f'../data/model_outputs/heatmaps/{MODEL_NAME}/{name}.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks = detect_peaks(source, name=name, min_distance=100, threshold_abs=0.95, save=False)\n",
    "candidate_map.add_data(data=peaks.copy(), name='Candidate Peaks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = detect_blobs(source, \n",
    "                     name=name, \n",
    "                     pred_threshold=0.9, \n",
    "                     min_sigma=5, \n",
    "                     area_threshold=0.0025,\n",
    "                     save=True)\n",
    "candidate_map.add_data(data=blobs.copy(), name='Candidate Blobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
