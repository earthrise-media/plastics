{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "import ee\n",
    "import geemap\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from scripts.get_s2_data_ee import get_history, get_history_polygon, get_pixel_vectors\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration:\n",
    "# Set directory where training site json files are located and files are saved\n",
    "# Set rect width for all patches that are not TPA sites\n",
    "DATA_DIR = '../data/training_sites'\n",
    "RECT_WIDTH = 0.002\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points(file_name):\n",
    "    \"\"\"Load points saved as a GeoJSON and return a dictionary\"\"\"\n",
    "    with open(os.path.join(DATA_DIR, file_name)) as f:\n",
    "        sites = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    site_table = pd.DataFrame({\n",
    "        'name': [file_name.split('_')[0] + '_' + str(index) for index in range(len(sites['features']))],\n",
    "        'lon': [site['geometry']['coordinates'][0] for site in sites['features']],\n",
    "        'lat': [site['geometry']['coordinates'][1] for site in sites['features']],\n",
    "        'coords': [site['geometry']['coordinates'][0:2] for site in sites['features']],\n",
    "    })\n",
    "    \n",
    "    return site_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_adjacent(tpa_sites, offset, direction='east'):\n",
    "    \"\"\"\n",
    "    Create a function that outputs a data frame of sampling locations based on a distance\n",
    "    and direction from each TPA site.\n",
    "    This can be used for adjacent site sampling, or to create \"random\" negative sites if the\n",
    "    offset distance is set further away from the TPA location.\n",
    "    Returns a data frame\n",
    "    \"\"\"\n",
    "    if  'east' in direction.lower():\n",
    "        adjacent_sites = pd.DataFrame({\n",
    "            'name': [f\"{name}_{direction.lower()}_{offset}\" for name in tpa_sites['name']],\n",
    "            'lon': [lon + offset for lon in tpa_sites['lon']],\n",
    "            'lat': [lat for lat in tpa_sites['lat']],\n",
    "            'coords': [[lon + offset, lat] for lon, lat in zip(tpa_sites['lon'], tpa_sites['lat'])]\n",
    "        })\n",
    "        \n",
    "    if  'west' in direction.lower():\n",
    "        adjacent_sites = pd.DataFrame({\n",
    "            'name': [f\"{name}_{direction.lower()}_{offset}\" for name in tpa_sites['name']],\n",
    "            'lon': [lon - offset for lon in tpa_sites['lon']],\n",
    "            'lat': [lat for lat in tpa_sites['lat']],\n",
    "            'coords': [[lon + offset, lat] for lon, lat in zip(tpa_sites['lon'], tpa_sites['lat'])]\n",
    "        })\n",
    "    \n",
    "    if  'north' in direction.lower():\n",
    "        adjacent_sites = pd.DataFrame({\n",
    "            'name': [f\"{name}_{direction.lower()}_{offset}\" for name in tpa_sites['name']],\n",
    "            'lon': [lon for lon in tpa_sites['lon']],\n",
    "            'lat': [lat + offset for lat in tpa_sites['lat']],\n",
    "            'coords': [[lon + offset, lat] for lon, lat in zip(tpa_sites['lon'], tpa_sites['lat'])]\n",
    "        })\n",
    "    \n",
    "    if  'south' in direction.lower():\n",
    "        adjacent_sites = pd.DataFrame({\n",
    "            'name': [f\"{name}_{direction.lower()}_{offset}\" for name in tpa_sites['name']],\n",
    "            'lon': [lon for lon in tpa_sites['lon']],\n",
    "            'lat': [lat - offset for lat in tpa_sites['lat']],\n",
    "            'coords': [[lon + offset, lat] for lon, lat in zip(tpa_sites['lon'], tpa_sites['lat'])]\n",
    "        })\n",
    "    \n",
    "    return adjacent_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TPA dataset\n",
    "with open(os.path.join(DATA_DIR, 'tpa_points.json')) as f:\n",
    "    tpa_points = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "tpa_sites = pd.DataFrame({\n",
    "    'name': [site['properties']['Name'] for site in tpa_points['features']],\n",
    "    'lon': [site['geometry']['coordinates'][0] for site in tpa_points['features']],\n",
    "    'lat': [site['geometry']['coordinates'][1] for site in tpa_points['features']],\n",
    "    'area': [site['properties']['Surface_Ha'] for site in tpa_points['features']],\n",
    "    'daily_volume': [site['properties']['TOT_Kg/Day'] for site in tpa_points['features']],\n",
    "    'coords': [site['geometry']['coordinates'] for site in tpa_points['features']]\n",
    "})\n",
    "\n",
    "\n",
    "# Add TPA Polygons to TPA dataframe\n",
    "with open(os.path.join(DATA_DIR, 'tpa_polygons', 'tpa_polygons.json'), 'r') as f:\n",
    "    json_tpa = json.load(f)\n",
    "f.close()\n",
    "tpa_polygons = [ee.FeatureCollection([element]) for element in list(json_tpa['features'])]\n",
    "tpa_sites['polygons'] = tpa_polygons\n",
    "display(tpa_sites.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dataframes for negative site sampling\n",
    "# Some lists are specifically chosen, and some are generated \n",
    "# automatically based on the locations of TPA sites\n",
    "negative_site_list = [load_points('city_points.json'), \n",
    "                      load_points('bare_earth_points.json'),\n",
    "                      sample_adjacent(tpa_sites, 0.008, 'north'),\n",
    "                      sample_adjacent(tpa_sites, 0.01, 'south'),\n",
    "                      sample_adjacent(tpa_sites, 0.01, 'east'),\n",
    "                      sample_adjacent(tpa_sites, 0.01, 'west'),\n",
    "                      sample_adjacent(tpa_sites, 0.05, 'east'),\n",
    "                      sample_adjacent(tpa_sites, 0.1, 'north'),\n",
    "                      sample_adjacent(tpa_sites, 0.2, 'west'),\n",
    "                      sample_adjacent(tpa_sites, 0.1, 'east'),\n",
    "                      sample_adjacent(tpa_sites, 0.5, 'east'),\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of patch histories\n",
    "# Each patch history is a dictionary with the format:\n",
    "# patch_history[date][site_name][band][band_img]\n",
    "# This function takes a while to run as it is extracting data from GEE\n",
    "patch_histories = []\n",
    "for site in negative_site_list:\n",
    "    patch_histories.append(get_history(site['coords'], site['name'], RECT_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose patch history into vectors\n",
    "# Output is site_type, month, pixel, band_value\n",
    "negative_pixel_vectors = []\n",
    "for patch_type in tqdm(patch_histories):\n",
    "    vectors = []\n",
    "    for month in patch_type.keys():\n",
    "        pixel_vectors, width, height = get_pixel_vectors(patch_type, month)\n",
    "        vectors.append(pixel_vectors)\n",
    "    negative_pixel_vectors.append(vectors)\n",
    "    \n",
    "# flatten all pixel_vectors into a flat set of vectors\n",
    "# num_vectors, num_bands\n",
    "negative_data = []\n",
    "for site_type in negative_pixel_vectors:\n",
    "    for month in site_type:\n",
    "        for pixel in month:\n",
    "            negative_data.append(pixel)\n",
    "            \n",
    "print(\"Number of Negative Train Samples:\", np.shape(negative_data)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get patch histories for TPA sites\n",
    "tpa_patch_histories = get_history_polygon(tpa_sites['coords'], tpa_sites['name'], tpa_sites['polygons'], 4 * RECT_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout_months refers to a strategy of holding out the last n months of data for validation\n",
    "# Set this value to the number of months you want to separate from the training data\n",
    "\n",
    "holdout_months = 2\n",
    "\n",
    "# decompose patch history into vectors\n",
    "# Output is site_type, month, pixel, band_value\n",
    "positive_pixel_vectors = []\n",
    "for month in list(tpa_patch_histories.keys())[:-holdout_months]:\n",
    "    pixel_vectors, width, height = get_pixel_vectors(tpa_patch_histories, month)\n",
    "    positive_pixel_vectors.append(pixel_vectors)\n",
    "    \n",
    "# flatten all pixel_vectors into a flat set of vectors\n",
    "# num_vectors, num_bands\n",
    "positive_train = []\n",
    "for month in positive_pixel_vectors:\n",
    "    for pixel in month:\n",
    "        positive_train.append(pixel)\n",
    "        \n",
    "print(\"Number of Positive Train Samples:\", np.shape(positive_train)[0])\n",
    "\n",
    "\n",
    "# decompose patch history into vectors\n",
    "# Output is site_type, month, pixel, band_value\n",
    "positive_pixel_vectors = []\n",
    "for month in list(tpa_patch_histories.keys())[-holdout_months:]:\n",
    "    pixel_vectors, width, height = get_pixel_vectors(tpa_patch_histories, month)\n",
    "    positive_pixel_vectors.append(pixel_vectors)\n",
    "    \n",
    "# flatten all pixel_vectors into a flat set of vectors\n",
    "# num_vectors, num_bands\n",
    "positive_test = []\n",
    "for month in positive_pixel_vectors:\n",
    "    for pixel in month:\n",
    "        positive_test.append(pixel)\n",
    "        \n",
    "print(\"Number of positive test samples:\", np.shape(positive_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save patch histories and pixel vectors\n",
    "\n",
    "f = open(os.path.join(DATA_DIR, \"negative_patch_histories.pkl\"),\"wb\")\n",
    "pickle.dump(patch_histories, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(DATA_DIR, \"tpa_patch_histories.pkl\"),\"wb\")\n",
    "pickle.dump(tpa_patch_histories, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(DATA_DIR, \"negative_data.pkl\"),\"wb\")\n",
    "pickle.dump(negative_data, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(DATA_DIR, \"positive_data.pkl\"),\"wb\")\n",
    "pickle.dump(positive_train, f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(DATA_DIR, \"positive_data_test.pkl\"),\"wb\")\n",
    "pickle.dump(positive_test, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_patch_history(data, name):\n",
    "    \"\"\"\n",
    "    Used for visualization and debugging. Takes a history dictionary and outputs a video\n",
    "    for each timestep at each site in the history.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(dpi=100, facecolor=(1,1,1))\n",
    "    ax.set_axis_off()\n",
    "    images = []\n",
    "    init_date = list(data.keys())[0]\n",
    "    for site_name in data[init_date]:\n",
    "        for date in data.keys():\n",
    "            ax.set_title(name)\n",
    "            hyperpatch = data[date][site_name]\n",
    "            rgb = np.stack((hyperpatch['B4'], hyperpatch['B3'], hyperpatch['B2']), axis=-1)\n",
    "            if len(rgb) > 0:\n",
    "                im = plt.imshow(rgb / 2000, animated=True)\n",
    "                images.append([im])\n",
    "    fig.tight_layout()\n",
    "    print(site_name.split('_')[1:])\n",
    "    ani = animation.ArtistAnimation(fig, images, interval=60, blit=True, repeat_delay=500)\n",
    "    ani.save(os.path.join('figures', 'videos', name + '.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_patch_history(tpa_patch_histories, 'TPA')\n",
    "animate_patch_history(patch_histories[0], 'city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pixel spectral profiles to observe any anomalies\n",
    "num_samples = 2000\n",
    "plt.figure(figsize=(8,5), dpi=150, facecolor=(1,1,1))\n",
    "for i in range(num_samples):\n",
    "    neg_index = np.random.randint(len(negative_data))\n",
    "    pos_index = np.random.randint(len(positive_test))\n",
    "    plt.plot(positive_test[i], c='r', alpha=0.01);\n",
    "    plt.plot(negative_data[i], c='k', alpha=0.01);\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color='r', lw=2),\n",
    "                Line2D([0], [0], color='k', lw=2)]\n",
    "plt.legend(custom_lines, ['TPA Test', 'Negative'], loc='upper left')\n",
    "plt.title('Spectral Profiles of Positive and Negative Samples')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5), dpi=150, facecolor=(1,1,1))\n",
    "for i in range(num_samples):\n",
    "    neg_index = np.random.randint(len(negative_data))\n",
    "    pos_index = np.random.randint(len(positive_train))\n",
    "    plt.plot(positive_train[i], c='r', alpha=0.01);\n",
    "    plt.plot(negative_data[i], c='k', alpha=0.01);\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color='r', lw=2),\n",
    "                Line2D([0], [0], color='k', lw=2)]\n",
    "plt.legend(custom_lines, ['TPA Train', 'Negative'], loc='upper left')\n",
    "plt.title('Spectral Profiles of Positive and Negative Samples')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
